{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.flirimageextractor import FlirImageExtractor\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "os.chdir(\"C:/Users/Bunni/Documents/FinalProject/ThermoML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLIR thermal images creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize FLIR extractor\n",
    "flir = FlirImageExtractor()\n",
    "folder_path = os.path.abspath('Data/images')\n",
    "output_dir = os.path.join(folder_path, \"magma\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file_path in glob.glob(os.path.join(folder_path, \"*.jpg\")):\n",
    "    # Load the thermal image\n",
    "    flir.process_image(file_path, True)\n",
    "    file = Path(file_path)\n",
    "\n",
    "    # Extract temperature matrix\n",
    "    thermal_data = flir.get_thermal_np()\n",
    "    thermal_gray = cv2.normalize(thermal_data, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    thermal_gray = thermal_gray.astype(np.uint8)\n",
    "\n",
    "    # Apply a colormap directly\n",
    "    thermal_color = cv2.applyColorMap(thermal_gray, cv2.COLORMAP_INFERNO)\n",
    "\n",
    "    #cv2.imwrite('new_image.jpg', thermal_gray) # Save the image\n",
    "    output_path = os.path.join(output_dir, f\"{file.name}.jpg\")\n",
    "    cv2.imwrite(output_path, thermal_color)\n",
    "\n",
    "    # Save temperature data as CSV\n",
    "    np.savetxt(\"temperature_data.csv\", thermal_data, delimiter=\",\")\n",
    "\n",
    "    # Show the thermal image\n",
    "    flir.save_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLIR optical images extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\o'\n",
      "C:\\Users\\Bunni\\AppData\\Local\\Temp\\ipykernel_16936\\3076532605.py:9: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  optical_path = folder_path + \"\\optical\\\\\" + \"optical_\" + file.name\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = os.path.abspath('Data/images')\n",
    "\n",
    "# In bash: $ exiftool -b -EmbeddedImage -42 Data/images/FLIR1231.jpg > optical_FLIR1231.jpg\n",
    "for file_path in glob.glob(f\"{folder_path}/*.jpg\"):\n",
    "    file = Path(file_path)\n",
    "    optical_path = folder_path + \"\\\\optical\\\\\" + \"optical_\" + file.name\n",
    "    subprocess.run([\"exiftool\", \"-b\", \"-EmbeddedImage\", \"-42\", file_path], stdout=open(optical_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.abspath('Data/images')\n",
    "\n",
    "folder_path = folder_path + \"/optical\"\n",
    "\n",
    "for file_path in glob.glob(f\"{folder_path}/*.jpg\"):\n",
    "    optical_img_gray = cv2.imread(optical_path, cv2.IMREAD_GRAYSCALE)\n",
    "    optical_img_color = cv2.imread(optical_path, cv2.IMREAD_ANYCOLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.abspath('Data/images')\n",
    "\n",
    "folder_path = folder_path + \"/bwr\"\n",
    "\n",
    "desired_size = (320, 240)  # (width, height)\n",
    "\n",
    "print(folder_path)\n",
    "# In bash: $ exiftool -b -EmbeddedImage -42 Data/images/FLIR1231.jpg > optical_FLIR1231.jpg\n",
    "for file_path in glob.glob(f\"{folder_path}/*.jpg\"):\n",
    "    thermal_img = cv2.imread(file_path, cv2.IMREAD_ANYCOLOR)\n",
    "    \n",
    "    # Resize only if needed\n",
    "    if thermal_img.shape[1] != desired_size[0] or thermal_img.shape[0] != desired_size[1]:\n",
    "        thermal_img = cv2.resize(thermal_img, desired_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    if thermal_img is None:\n",
    "       print(\"Error: Optical image extraction failed!\")\n",
    "    else:\n",
    "        cv2.imshow(\"Optical Image\", thermal_img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry\n",
    "import torch\n",
    "\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=\"C:/Users/Bunni/Documents/FinalProject/ThermoML/models/sam_vit_h_4b8939.pth\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we separate thermal pictures that were extracted using the thermal data with the cv2 inferno color using the Meta SAM model. We get a segmentation of each hand separately.\n",
    "\n",
    "We achieve successful(both hands) segmentation at 70%. This rate includes cold hands that struggle to be detected, excluding cold hands the rate is bigger around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# === Enhance contrast ===\n",
    "def enhance_contrast(image):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "# === Improved feathering ===\n",
    "def feather_edges(image, mask, feather_amount=5):\n",
    "    mask = mask.astype(np.float32)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    mask_dilated = cv2.dilate(mask, kernel, iterations=1)\n",
    "    edge = mask_dilated - mask\n",
    "    edge_blur = cv2.GaussianBlur(edge, (0, 0), feather_amount)\n",
    "    soft_mask = np.clip(mask + edge_blur, 0, 1)\n",
    "    alpha = np.stack([soft_mask] * 3, axis=-1)\n",
    "    blended = (image * alpha).astype(np.uint8)\n",
    "    return blended\n",
    "\n",
    "# === Settings ===\n",
    "project_dir = \"C:/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "inferno_dir = os.path.join(project_dir, \"data\", \"images\", \"inferno\")\n",
    "desired_size = (320, 240)\n",
    "min_area = 1000\n",
    "\n",
    "# === Load SAM ===\n",
    "sam = sam_model_registry[\"vit_h\"](\n",
    "    checkpoint=os.path.join(project_dir, \"backend\", \"models\", \"sam_vit_h_4b8939.pth\")\n",
    ").to(\"cuda\")\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# === Output directory ===\n",
    "output_dir = os.path.join(project_dir, \"inferno_output\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"\\n Processing inferno images in: {inferno_dir}\")\n",
    "\n",
    "# === Process only inferno images ===\n",
    "for file_path in glob.glob(f\"{inferno_dir}/*.jpg\"):\n",
    "    filename = os.path.basename(file_path)\n",
    "    print(f\"\\n Processing image: {filename}\")\n",
    "\n",
    "    image = cv2.imread(file_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if image.shape[1] != desired_size[0] or image.shape[0] != desired_size[1]:\n",
    "        image = cv2.resize(image, desired_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    image = enhance_contrast(image)\n",
    "    predictor.set_image(image)\n",
    "\n",
    "    h, w = desired_size[1], desired_size[0]\n",
    "    input_point = np.array([\n",
    "        [int(w * 0.25), int(h * 0.6)],\n",
    "        [int(w * 0.75), int(h * 0.6)],\n",
    "        [int(w * 0.25), int(h * 0.4)],\n",
    "        [int(w * 0.75), int(h * 0.4)],\n",
    "        [int(w * 0.5), int(h * 0.85)]\n",
    "    ])\n",
    "    input_label = np.array([1, 1, 1, 1, 0])\n",
    "\n",
    "    masks, _, _ = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=False\n",
    "    )\n",
    "\n",
    "    mask = masks[0].astype(np.uint8)\n",
    "    num_labels, labels = cv2.connectedComponents(mask)\n",
    "    print(f\" Found {num_labels - 1} components.\")\n",
    "\n",
    "    hands = []\n",
    "    for label in range(1, num_labels):\n",
    "        hand_mask = (labels == label).astype(np.uint8)\n",
    "        area = cv2.countNonZero(hand_mask)\n",
    "        if area < min_area:\n",
    "            continue\n",
    "        M = cv2.moments(hand_mask)\n",
    "        if M[\"m00\"] == 0:\n",
    "            continue\n",
    "        center_x = int(M[\"m10\"] / M[\"m00\"])\n",
    "        hands.append((center_x, hand_mask))\n",
    "\n",
    "    if len(hands) == 1:\n",
    "        print(\"Only 1 component found attempting to split\")\n",
    "        big_mask = hands[0][1]\n",
    "        mid_x = w // 2\n",
    "        left_mask = np.zeros_like(big_mask)\n",
    "        right_mask = np.zeros_like(big_mask)\n",
    "        left_mask[:, :mid_x] = big_mask[:, :mid_x]\n",
    "        right_mask[:, mid_x:] = big_mask[:, mid_x:]\n",
    "        if cv2.countNonZero(left_mask) > min_area and cv2.countNonZero(right_mask) > min_area:\n",
    "            hands = [(int(w * 0.25), left_mask), (int(w * 0.75), right_mask)]\n",
    "        else:\n",
    "            print(\"Failed to split  skipping image.\")\n",
    "            continue\n",
    "\n",
    "    if len(hands) != 2:\n",
    "        print(\"Skipping did not find exactly 2 hands.\")\n",
    "        continue\n",
    "\n",
    "    hands.sort(key=lambda h: h[0])\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "\n",
    "    for i, (center_x, hand_mask) in enumerate(hands):\n",
    "        side = \"left\" if i == 0 else \"right\"\n",
    "        feathered_hand = feather_edges(image, hand_mask, feather_amount=5)\n",
    "        output_path = os.path.join(output_dir, f\"hand_{side}_{base_name}.jpg\")\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(feathered_hand, cv2.COLOR_RGB2BGR))\n",
    "        print(f\" Saved {side} hand to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we separate the segmented hands that we got from the SAM into the joints, and the tips of the fingers using the google HandLandMarker model.\n",
    "\n",
    "We achieve successful joint recognition at 100% of all successful segmented hands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1231.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1232.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1233.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1234.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1235.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1236.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1237.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1238.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1239.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1240.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1241.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1242.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1243.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1244.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1245.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1246.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1247.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1248.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1249.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1250.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1251.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1252.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1253.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1254.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1255.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1256.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1257.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1258.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1259.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1260.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1261.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1262.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1263.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1264.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1265.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1266.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1267.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1272.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1273.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1274.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1275.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1276.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1278.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1279.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1280.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1281.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1282.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1283.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1284.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1285.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1286.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1287.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1288.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1289.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1290.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1291.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1292.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1293.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1294.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1295.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1296.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1297.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1298.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1299.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1300.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1301.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1302.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1303.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1304.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1305.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1306.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1307.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1308.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1309.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1310.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1311.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1312.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1313.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1314.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1315.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1316.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1317.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1318.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1319.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1320.jpg\n",
      "Saved landmark image to: C:/Users/Bunni/Documents/FinalProject/ThermoML\\inferno_landmarks_reversy\\landmarks_FLIR1321.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# === Paths ===\n",
    "project_dir = \"C:/Users/Bunni/Documents/FinalProject/ThermoML/backend\"\n",
    "input_dir = os.path.join(project_dir, \"optical_output\")\n",
    "output_dir = os.path.join(project_dir, \"optical_landmarks\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Load MediaPipe model ===\n",
    "model_path = os.path.join(project_dir, \"backend\", \"models\", \"hand_landmarker.task\")\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    num_hands=2\n",
    ")\n",
    "\n",
    "landmarker = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# === Process each image ===\n",
    "for file in os.listdir(input_dir):\n",
    "    if not file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        continue \n",
    "\n",
    "    image_path = os.path.join(input_dir, file)\n",
    "    image = cv2.imread(image_path)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)\n",
    "\n",
    "    result = landmarker.detect(mp_image)\n",
    "\n",
    "    if result.hand_landmarks:\n",
    "        for hand in result.hand_landmarks:\n",
    "            for landmark in hand:\n",
    "                x_px = int(landmark.x * image.shape[1])\n",
    "                y_px = int(landmark.y * image.shape[0])\n",
    "                cv2.circle(image, (x_px, y_px), 3, (0, 255, 0), -1)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"landmarks_{file}\")\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Saved landmark image to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "project_dir = \"C:/Users/Bunni/Documents/FinalProject/ThermoML/\"\n",
    "\n",
    "# Load Excel files\n",
    "df_images = pd.read_excel(os.path.join(project_dir, \"Data\", \"TH-SRG01 FLIR numbers May2025.xlsx\"))\n",
    "df_labels = pd.read_excel(os.path.join(project_dir, \"Data\", \"TH-SRG01 Investigator Database 16May25.xlsx\"))\n",
    "\n",
    "# Define the exact column order based on your description\n",
    "label_columns = [\n",
    "    \"TENDERNESS_WRIST_RIGHT\", \"TENDERNESS_WRIST_LEFT\", \"TENDERNESS_CMC1_RIGHT\", \"TENDERNESS_CMC1_LEFT\",\n",
    "    \"TENDERNESS_MCP1_RIGHT\", \"TENDERNESS_MCP1_LEFT\", \"TENDERNESS_IP1_RIGHT\", \"TENDERNESS_IP1_LEFT\",\n",
    "    \"TENDERNESS_MCP2_RIGHT\", \"TENDERNESS_MCP2_LEFT\", \"TENDERNESS_PIP2_RIGHT\", \"TENDERNESS_PIP2_LEFT\",\n",
    "    \"TENDERNESS_DIP2_RIGHT\", \"TENDERNESS_DIP2_LEFT\", \"TENDERNESS_MCP3_RIGHT\", \"TENDERNESS_MCP3_LEFT\",\n",
    "    \"TENDERNESS_PIP3_RIGHT\", \"TENDERNESS_PIP3_LEFT\", \"TENDERNESS_DIP3_RIGHT\", \"TENDERNESS_DIP3_LEFT\",\n",
    "    \"TENDERNESS_MCP4_RIGHT\", \"TENDERNESS_MCP4_LEFT\", \"TENDERNESS_PIP4_RIGHT\", \"TENDERNESS_PIP4_LEFT\",\n",
    "    \"TENDERNESS_DIP4_RIGHT\", \"TENDERNESS_DIP4_LEFT\", \"TENDERNESS_MCP5_RIGHT\", \"TENDERNESS_MCP5_LEFT\",\n",
    "    \"TENDERNESS_PIP5_RIGHT\", \"TENDERNESS_PIP5_LEFT\", \"TENDERNESS_DIP5_RIGHT\", \"TENDERNESS_DIP5_LEFT\"\n",
    "]\n",
    "\n",
    "# Extract label columns in order, resulting in shape (num_rows, 32)\n",
    "labels = df[label_columns].to_numpy()\n",
    "\n",
    "# Access example:\n",
    "# labels[0][0] is the value of TENDERNESS_WRIST_RIGHT in the first (sorted) row\n",
    "\n",
    "\n",
    "# --- Rename 'Code' column to match ---\n",
    "df_labels.rename(columns={\"code\": \"Patient Code\"}, inplace=True)\n",
    "\n",
    "# --- Define tenderness columns to keep ---\n",
    "target_tenderness_cols = [\n",
    "    \"TENDERNESS_WRIST_RIGHT\", \"TENDERNESS_WRIST_LEFT\",\n",
    "    \"TENDERNESS_CMC1_RIGHT\", \"TENDERNESS_CMC1_LEFT\",\n",
    "    \"TENDERNESS_MCP1_RIGHT\", \"TENDERNESS_MCP1_LEFT\",\n",
    "    \"TENDERNESS_IP1_RIGHT\", \"TENDERNESS_IP1_LEFT\",\n",
    "    \"TENDERNESS_MCP2_RIGHT\", \"TENDERNESS_MCP2_LEFT\",\n",
    "    \"TENDERNESS_PIP2_RIGHT\", \"TENDERNESS_PIP2_LEFT\",\n",
    "    \"TENDERNESS_DIP2_RIGHT\", \"TENDERNESS_DIP2_LEFT\",\n",
    "    \"TENDERNESS_MCP3_RIGHT\", \"TENDERNESS_MCP3_LEFT\",\n",
    "    \"TENDERNESS_PIP3_RIGHT\", \"TENDERNESS_PIP3_LEFT\",\n",
    "    \"TENDERNESS_DIP3_RIGHT\", \"TENDERNESS_DIP3_LEFT\",\n",
    "    \"TENDERNESS_MCP4_RIGHT\", \"TENDERNESS_MCP4_LEFT\",\n",
    "    \"TENDERNESS_PIP4_RIGHT\", \"TENDERNESS_PIP4_LEFT\",\n",
    "    \"TENDERNESS_DIP4_RIGHT\", \"TENDERNESS_DIP4_LEFT\",\n",
    "    \"TENDERNESS_MCP5_RIGHT\", \"TENDERNESS_MCP5_LEFT\",\n",
    "    \"TENDERNESS_PIP5_RIGHT\", \"TENDERNESS_PIP5_LEFT\",\n",
    "    \"TENDERNESS_DIP5_RIGHT\", \"TENDERNESS_DIP5_LEFT\"\n",
    "]\n",
    "\n",
    "df_labels[target_tenderness_cols] = df_labels[target_tenderness_cols].replace(\".\", 0).fillna(0).astype(int)\n",
    "\n",
    "# --- Enhance tenderness labels using OR logic ---\n",
    "# CMC1 (Right and Left)\n",
    "df_labels[\"TENDERNESS_CMC1_RIGHT\"] = (\n",
    "    (df_labels[\"CMC1_GRIND_RIGHT\"] == 1) | (df_labels[\"TENDERNESS_CMC1_RIGHT\"] == 1)\n",
    ").astype(int)\n",
    "\n",
    "df_labels[\"TENDERNESS_CMC1_LEFT\"] = (\n",
    "    (df_labels[\"CMC1_GRIND_LEFT\"] == 1) | (df_labels[\"TENDERNESS_CMC1_LEFT\"] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# MCP1 (Right and Left)\n",
    "df_labels[\"TENDERNESS_MCP1_RIGHT\"] = (\n",
    "    (df_labels[\"MCP1_INSTABILITY_RIGHT\"] == 1) | (df_labels[\"TENDERNESS_MCP1_RIGHT\"] == 1)\n",
    ").astype(int)\n",
    "\n",
    "df_labels[\"TENDERNESS_MCP1_LEFT\"] = (\n",
    "    (df_labels[\"MCP1_INSTABILITY_LEFT\"] == 1) | (df_labels[\"TENDERNESS_MCP1_LEFT\"] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# Wrist (Right and Left)\n",
    "df_labels[\"TENDERNESS_WRIST_RIGHT\"] = (\n",
    "    (df_labels[\"TENDERNESS_WRIST_RADIAL_RIGHT\"] == 1) |\n",
    "    (df_labels[\"TENDERNESS_WRIST_ULNAR_RIGHT\"] == 1) |\n",
    "    (df_labels[\"סמן אם יש: | FINKLESTEIN TEST: DQ | יד ימין\"] == 1) |\n",
    "    (df_labels[\"סמן אם יש: | ECU TENDERNESS | יד ימין\"] == 1)\n",
    ").astype(int)\n",
    "\n",
    "df_labels[\"TENDERNESS_WRIST_LEFT\"] = (\n",
    "    (df_labels[\"TENDERNESS_WRIST_RADIAL_LEFT\"] == 1) |\n",
    "    (df_labels[\"TENDERNESS_WRIST_ULNAR_LEFT\"] == 1) |\n",
    "    (df_labels[\"סמן אם יש: | FINKLESTEIN TEST: DQ | יד שמאל\"] == 1) |\n",
    "    (df_labels[\"סמן אם יש: | ECU TENDERNESS | יד שמאל\"] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# --- Filter columns to keep ---\n",
    "df_labels_filtered = df_labels[[\"Patient Code\"] + target_tenderness_cols]\n",
    "\n",
    "# --- Merge ---\n",
    "df_merged = pd.merge(df_images, df_labels_filtered, on=\"Patient Code\", how=\"inner\")\n",
    "\n",
    "# --- Save ---\n",
    "output_path = os.path.join(project_dir, \"Data\", \"merged_inflammation_dataset.csv\")\n",
    "df_merged.to_csv(output_path, index=False)\n",
    "print(\"Saved to:\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out rows that are not in image database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('your_file.csv')\n",
    "\n",
    "# Folder with your actual images\n",
    "image_folder = 'path_to_images'\n",
    "\n",
    "available_flir_ids = set()\n",
    "\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.startswith('masked_FLIR') and filename.endswith(('.jpg', '.png')):\n",
    "        match = re.search(r'masked_FLIR(\\d+)', filename)\n",
    "        if match:\n",
    "            flir_id = int(match.group(1))\n",
    "            available_flir_ids.add(flir_id)\n",
    "\n",
    "# Filter DataFrame to only include rows with FLIR IDs present in the image folder\n",
    "df = df[df['FLIR'].isin(available_flir_ids)].sort_values(by='FLIR').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordering landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reorder_landmarks_if_needed(landmarks):\n",
    "    \"\"\"\n",
    "    Ensures that landmarks of the left hand (with smaller x) come before the right hand.\n",
    "    Assumes:\n",
    "    - landmarks is a (42, 2) or (42, 3) array.\n",
    "    - landmarks[0:21] are one hand, landmarks[21:42] are the other.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if we have exactly 42 landmarks (2 hands)\n",
    "    if landmarks.shape[0] != 42:\n",
    "        return landmarks  # No change needed\n",
    "\n",
    "    left_hand = landmarks[:21]\n",
    "    right_hand = landmarks[21:]\n",
    "\n",
    "    # Compare x-coordinates (column 0) of landmark 0 and 21\n",
    "    if landmarks[0][0] > landmarks[21][0]:  # Left hand is actually second in the array\n",
    "        landmarks = np.concatenate([right_hand, left_hand], axis=0)\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "def remap_landmarks(landmarks):\n",
    "    \"\"\"\n",
    "    Reorders landmarks according to a custom label mapping and removes irrelevant points.\n",
    "\n",
    "    Args:\n",
    "        landmarks (np.ndarray): Original array of shape (42, N) where N=2 or 3 (x, y, [z])\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Remapped landmark array of shape (32, N)\n",
    "    \"\"\"\n",
    "    # Irrelevant landmarks to remove\n",
    "    irrelevant_indices = {4, 8, 12, 16, 20, 25, 29, 33, 37, 41}\n",
    "\n",
    "    # Mapping: new_index -> old_index\n",
    "    mapping = [\n",
    "        21, 0, 22, 1, 23, 2, 24, 3,\n",
    "        26, 5, 27, 6, 28, 7, 30, 9,\n",
    "        31, 10, 32, 11, 34, 13, 35, 14,\n",
    "        36, 15, 38, 17, 39, 18, 40, 19\n",
    "    ]\n",
    "\n",
    "    # Apply mapping and ignore irrelevant indices\n",
    "    filtered_landmarks = np.array([landmarks[i] for i in mapping if i not in irrelevant_indices])\n",
    "\n",
    "    return filtered_landmarks\n",
    "\n",
    "all_landmarks = np.load(os.path.join(project_dir, \"landmarks_optical\", \"all_optical_landmarks.npy\"))\n",
    "reordered_landmarks = np.array([\n",
    "    reorder_landmarks_if_needed(landmarks)\n",
    "    for landmarks in all_landmarks\n",
    "])\n",
    "\n",
    "# Optional: Save or use reordered_landmarks here\n",
    "# np.save(os.path.join(project_dir, \"reordered_landmarks.npy\"), reordered_landmarks)\n",
    "\n",
    "# === Step 2: Remap after reorder ===\n",
    "remapped_landmarks = np.array([\n",
    "    remap_landmarks(landmarks)\n",
    "    for landmarks in reordered_landmarks\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (4, 240, 320)\n",
      "Dtype: float32\n",
      "\n",
      "thermal channel:\n",
      "  Shape: (240, 320)\n",
      "  Dtype: float32\n",
      "  Unique values: [0.         0.00414938 0.00829876 0.01244813 0.01659751 0.02074689\n",
      " 0.02489627 0.02904564 0.03319502 0.0373444  0.04149378 0.04564315\n",
      " 0.04979253 0.05394191 0.05809129 0.06224066 0.06639004 0.07053942\n",
      " 0.0746888  0.07883818 0.08298755 0.08713693 0.09128631 0.09543569\n",
      " 0.09958506 0.10373444 0.10788382 0.1120332  0.11618257 0.12033195\n",
      " 0.12448133 0.12863071 0.13278009 0.13692947 0.14107884 0.14522822\n",
      " 0.1493776  0.15352698 0.15767635 0.16182573 0.16597511 0.17012449\n",
      " 0.17427386 0.17842324 0.18257262 0.186722   0.19087137 0.19502075\n",
      " 0.19917013 0.2033195  0.20746888 0.21161826 0.21576764 0.21991701\n",
      " 0.22406639 0.22821577 0.23236515 0.23651452 0.2406639  0.24481328\n",
      " 0.24896266 0.25311205 0.25726143 0.2614108  0.26556018 0.26970956\n",
      " 0.27385893 0.2780083  0.2821577  0.28630707 0.29045644 0.29460582\n",
      " 0.2987552  0.30290458 0.30705395 0.31120333 0.3153527  0.3195021\n",
      " 0.32365146 0.32780084 0.33195022 0.3360996  0.34024897 0.34439835\n",
      " 0.34854773 0.3526971  0.35684648 0.36099586 0.36514524 0.3692946\n",
      " 0.373444   0.37759337 0.38174275 0.38589212 0.3900415  0.39419088\n",
      " 0.39834026 0.40248963 0.406639   0.4107884  0.41493776 0.41908714\n",
      " 0.42323652 0.4273859  0.43153527 0.43568465 0.43983403 0.4439834\n",
      " 0.44813278 0.45228216 0.45643154 0.46058092 0.4647303  0.46887967\n",
      " 0.47302905 0.47717842 0.4813278  0.48547718 0.48962656 0.49377593\n",
      " 0.4979253  0.5020747  0.5062241  0.5103735  0.51452285 0.5186722\n",
      " 0.5228216  0.526971   0.53112036 0.53526974 0.5394191  0.5435685\n",
      " 0.54771787 0.55186725 0.5560166  0.560166   0.5643154  0.56846476\n",
      " 0.57261413 0.5767635  0.5809129  0.58506227 0.58921164 0.593361\n",
      " 0.5975104  0.6016598  0.60580915 0.6099585  0.6141079  0.6182573\n",
      " 0.62240666 0.62655604 0.6307054  0.6348548  0.6390042  0.64315355\n",
      " 0.6473029  0.6514523  0.6556017  0.65975106 0.66390043 0.6680498\n",
      " 0.6721992  0.67634857 0.68049794 0.6846473  0.6887967  0.6929461\n",
      " 0.69709545 0.70124483 0.7053942  0.7095436  0.71369296 0.71784234\n",
      " 0.7219917  0.7261411  0.7302905  0.73443985 0.7385892  0.7427386\n",
      " 0.746888   0.75103736 0.75518674 0.7593361  0.7634855  0.76763487\n",
      " 0.77178425 0.7759336  0.780083   0.7842324  0.78838176 0.79253113\n",
      " 0.7966805  0.8008299  0.80497926 0.80912864 0.813278   0.8174274\n",
      " 0.8215768  0.82572615 0.8298755  0.8340249  0.8381743  0.84232366\n",
      " 0.84647304 0.8506224  0.8547718  0.8589212  0.86307055 0.8672199\n",
      " 0.8713693  0.8755187  0.87966806 0.88381743 0.8879668  0.8921162\n",
      " 0.89626557 0.90041494 0.9045643  0.9087137  0.9128631  0.91701245\n",
      " 0.92116183 0.9253112  0.9294606  0.93360996 0.93775934 0.9419087\n",
      " 0.9460581  0.9502075  0.95435685 0.9585062  0.9626556  0.966805\n",
      " 0.97095436 0.97510374 0.9792531  0.9834025  0.98755187 0.99170125\n",
      " 0.9958506  1.        ]\n",
      "\n",
      "mask channel:\n",
      "  Shape: (240, 320)\n",
      "  Dtype: float32\n",
      "  Unique values: [0. 1.]\n",
      "\n",
      "masked_thermal channel:\n",
      "  Shape: (240, 320)\n",
      "  Dtype: float32\n",
      "  Unique values: [0.         0.5228216  0.526971   0.53526974 0.5394191  0.5435685\n",
      " 0.54771787 0.55186725 0.5560166  0.560166   0.5643154  0.56846476\n",
      " 0.57261413 0.5767635  0.5809129  0.58506227 0.58921164 0.593361\n",
      " 0.5975104  0.6016598  0.60580915 0.6099585  0.6141079  0.6182573\n",
      " 0.62240666 0.62655604 0.6307054  0.6348548  0.6390042  0.64315355\n",
      " 0.6473029  0.6514523  0.6556017  0.65975106 0.66390043 0.6680498\n",
      " 0.6721992  0.67634857 0.68049794 0.6846473  0.6887967  0.6929461\n",
      " 0.69709545 0.70124483 0.7053942  0.7095436  0.71369296 0.71784234\n",
      " 0.7219917  0.7261411  0.7302905  0.73443985 0.7385892  0.7427386\n",
      " 0.746888   0.75103736 0.75518674 0.7593361  0.7634855  0.76763487\n",
      " 0.77178425 0.7759336  0.780083   0.7842324  0.78838176 0.79253113\n",
      " 0.7966805  0.8008299  0.80497926 0.80912864 0.813278   0.8174274\n",
      " 0.8215768  0.82572615 0.8298755  0.8340249  0.8381743  0.84232366\n",
      " 0.84647304 0.8506224  0.8547718  0.8589212  0.86307055 0.8672199\n",
      " 0.8713693  0.8755187  0.87966806 0.88381743 0.8879668  0.8921162\n",
      " 0.89626557 0.90041494 0.9045643  0.9087137  0.9128631  0.91701245\n",
      " 0.92116183 0.9253112  0.9294606  0.93360996 0.93775934 0.9419087\n",
      " 0.9460581  0.9502075  0.95435685 0.9585062  0.9626556  0.966805\n",
      " 0.97095436 0.97510374 0.9792531  0.9834025  0.98755187 0.99170125\n",
      " 0.9958506  1.        ]\n",
      "\n",
      "distance_map channel:\n",
      "  Shape: (240, 320)\n",
      "  Dtype: float32\n",
      "  Unique values: [0.         0.0058343  0.00825095 ... 0.9953428  0.9965049  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def prepare_4_channel_image(thermal_image, segmentation_mask, hand_centers=None):\n",
    "    if thermal_image.ndim == 3 and thermal_image.shape[2] == 3:\n",
    "        pil_gray = Image.fromarray(thermal_image).convert(\"L\")\n",
    "        thermal_image = np.array(pil_gray).astype(np.float32)\n",
    "    elif thermal_image.ndim == 2:\n",
    "        thermal_image = thermal_image.astype(np.float32)\n",
    "    else:\n",
    "        raise ValueError(\"Expected RGB or 2D image\")\n",
    "\n",
    "    # Normalize\n",
    "    thermal = (thermal_image - thermal_image.min()) / (thermal_image.max() - thermal_image.min() + 1e-6)\n",
    "    thermal = torch.tensor(thermal, dtype=torch.float32)\n",
    "    mask = torch.tensor(segmentation_mask, dtype=torch.float32)\n",
    "    masked_thermal = thermal * mask\n",
    "    H, W = thermal.shape\n",
    "\n",
    "    if hand_centers is not None and len(hand_centers) > 0:\n",
    "        y_grid, x_grid = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
    "        distances = [torch.sqrt((x_grid - cx)**2 + (y_grid - cy)**2) for cx, cy in hand_centers]\n",
    "        distance_map = torch.min(torch.stack(distances, dim=0), dim=0).values\n",
    "        distance_map = distance_map / (distance_map.max() + 1e-6)\n",
    "    else:\n",
    "        distance_map = torch.nn.functional.avg_pool2d(mask.unsqueeze(0).unsqueeze(0), kernel_size=11, stride=1, padding=5).squeeze()\n",
    "\n",
    "    image_4ch = torch.stack([thermal, mask, masked_thermal, distance_map], dim=0)\n",
    "    return image_4ch\n",
    "\n",
    "# No need for this function but it's kept here \n",
    "def clean_mask(mask: np.ndarray, kernel_size=5):\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    cleaned = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)  # removes small noise\n",
    "    cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, kernel)  # fills small holes\n",
    "    return cleaned\n",
    "\n",
    "def get_hand_centers(landmarks):\n",
    "    centers = []\n",
    "    centers.append(intersect(landmarks[1], landmarks[17], landmarks[12], landmarks[0]))\n",
    "    centers.append(intersect(landmarks[22], landmarks[38], landmarks[33], landmarks[21]))\n",
    "\n",
    "def intersect(p1, p2, p3, p4):\n",
    "    a1 = p2[1] - p1[1]\n",
    "    b1 = p1[0] - p2[0]\n",
    "    c1 = a1 * p1[0] + b1 * p1[1]\n",
    "\n",
    "    a2 = p4[1] - p3[1]\n",
    "    b2 = p3[0] - p4[0]\n",
    "    c2 = a2 * p3[0] + b2 * p3[1]\n",
    "\n",
    "    determinant = a1 * b2 - a2 * b1\n",
    "    if determinant == 0:\n",
    "        return None\n",
    "    x = (b2 * c1 - b1 * c2) / determinant\n",
    "    y = (a1 * c2 - a2 * c1) / determinant\n",
    "    return int(x), int(y)\n",
    "\n",
    "# Transform mask image into binary mask for second channel\n",
    "# In actual use, using 1/8 of the max value in the image or lower is working well\n",
    "def load_segmented_image_as_mask(path, threshold=128):\n",
    "    # Load as grayscale (0–255)\n",
    "    seg_img = Image.open(path).convert(\"L\")\n",
    "    seg_array = np.array(seg_img)\n",
    "\n",
    "    # Convert to binary mask: hand = 1, background = 0\n",
    "    binary_mask = (seg_array > threshold).astype(np.uint8)\n",
    "    return binary_mask  # Shape: (H, W), values 0 or 1\n",
    "\n",
    "import os\n",
    "\n",
    "# === Paths ===\n",
    "project_dir = \"C:/Users/Bunni/Documents/FinalProject/ThermoML/\"\n",
    "input_dir = os.path.join(project_dir, \"backend\", \"inferno_landmarks_combined\")\n",
    "output_dir = os.path.join(project_dir, \"backend\", \"inferno_landmarks_morph\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Process each image ===\n",
    "for file in os.listdir(input_dir):\n",
    "    if not file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        continue \n",
    "\n",
    "    image_path = os.path.join(input_dir, file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    #cleaned_image = clean_mask(image)\n",
    "    binary_image = load_segmented_image_as_mask(image_path, np.max(image) // 8)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"clean_{file}\")\n",
    "    cv2.imwrite(output_path, binary_image)\n",
    "    print(f\"Saved landmark image to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Landmarks JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# === Paths ===\n",
    "project_dir = \"C:/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "input_dir = os.path.join(project_dir, \"Data\", \"images\", \"inferno\")\n",
    "json_output_dir = os.path.join(project_dir, \"inferno_jsons\")\n",
    "os.makedirs(json_output_dir, exist_ok=True)\n",
    "\n",
    "# === Load MediaPipe model ===\n",
    "model_path = os.path.join(project_dir, \"backend\", \"models\", \"hand_landmarker.task\")\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    num_hands=2\n",
    ")\n",
    "landmarker = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# === Process each image ===\n",
    "for file in os.listdir(input_dir):\n",
    "    if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue\n",
    "\n",
    "    image_path = os.path.join(input_dir, file)\n",
    "    image = cv2.imread(image_path)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)\n",
    "\n",
    "    result = landmarker.detect(mp_image)\n",
    "\n",
    "    if result.hand_landmarks:\n",
    "        hands_data = []\n",
    "        for hand_landmarks in result.hand_landmarks:\n",
    "            points = [{\"x\": float(l.x), \"y\": float(l.y), \"z\": float(l.z)} for l in hand_landmarks]\n",
    "            avg_x = sum(p[\"x\"] for p in points) / len(points)\n",
    "            hands_data.append((avg_x, points))\n",
    "\n",
    "        # Sort hands by x (from left to right on the image)\n",
    "        hands_data.sort(key=lambda h: h[0])\n",
    "\n",
    "        # Assign correct hand labels (left hand appears on left of image)\n",
    "        labels = [\"left\", \"right\"]\n",
    "\n",
    "        base_name = os.path.splitext(file)[0]\n",
    "\n",
    "        for i, (_, landmarks) in enumerate(hands_data):\n",
    "            json_filename = f\"{base_name}_{labels[i]}_hand.json\"\n",
    "            json_path = os.path.join(json_output_dir, json_filename)\n",
    "\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(landmarks, f, indent=4)\n",
    "\n",
    "            print(f\"✅ Saved {labels[i]} hand JSON to: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Landmarks model that checks how many landmarks on each hand and saves accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "project_dir = \"C:/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "input_dir = os.path.join(project_dir, \"optical_output\", \"inferno_output\")\n",
    "output_dir = os.path.join(project_dir, \"backend\", \"optical_landmarks_combined\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Load MediaPipe model ===\n",
    "model_path = os.path.join(project_dir, \"backend\", \"models\", \"hand_landmarker.task\")\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    num_hands=2\n",
    ")\n",
    "\n",
    "landmarker = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# === Helper function to mark landmarks ===\n",
    "def draw_landmarks_on_image(image):\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)\n",
    "    result = landmarker.detect(mp_image)\n",
    "\n",
    "    if result.hand_landmarks:\n",
    "        for hand in result.hand_landmarks:\n",
    "            for landmark in hand:\n",
    "                x_px = int(landmark.x * image.shape[1])\n",
    "                y_px = int(landmark.y * image.shape[0])\n",
    "                cv2.circle(image, (x_px, y_px), 3, (0, 255, 0), -1)\n",
    "        return image, sum(len(hand) for hand in result.hand_landmarks)\n",
    "    return image, 0\n",
    "\n",
    "# === Process segmented images ===\n",
    "segmented_files = [f for f in os.listdir(input_dir) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
    "\n",
    "# Group left/right hand pairs\n",
    "base_names = set()\n",
    "for file in segmented_files:\n",
    "    if \"hand_left\" in file or \"hand_right\" in file:\n",
    "        base = file.replace(\"hand_left_\", \"\").replace(\"hand_right_\", \"\")\n",
    "        base_names.add(base)\n",
    "    else:\n",
    "        base_names.add(file)\n",
    "\n",
    "for base in base_names:\n",
    "    left_path = os.path.join(input_dir, f\"hand_left_{base}\")\n",
    "    right_path = os.path.join(input_dir, f\"hand_right_{base}\")\n",
    "    both_path = os.path.join(input_dir, base)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"landmarks_{base}\")\n",
    "    \n",
    "    # Case 2: Process left + right if both exist\n",
    "    has_left = os.path.exists(left_path)\n",
    "    has_right = os.path.exists(right_path)\n",
    "    has_base = os.path.exists(both_path)\n",
    "\n",
    "    if has_left and has_right:\n",
    "        left_img = cv2.imread(left_path)\n",
    "        right_img = cv2.imread(right_path)\n",
    "\n",
    "        left_marked, left_points = draw_landmarks_on_image(left_img)\n",
    "        right_marked, right_points = draw_landmarks_on_image(right_img)\n",
    "\n",
    "        combined = cv2.add(left_marked, right_marked)\n",
    "\n",
    "        if left_points == 21 and right_points == 21:\n",
    "            # Resize to match height\n",
    "            # height = max(left_marked.shape[0], right_marked.shape[0])\n",
    "            # left_resized = cv2.resize(left_marked, (left_marked.shape[1], height))\n",
    "            # right_resized = cv2.resize(right_marked, (right_marked.shape[1], height))\n",
    "\n",
    "            cv2.imwrite(output_path, combined)\n",
    "            print(f\"Saved COMBINED hands image: {output_path}\")\n",
    "        elif left_points == 42:\n",
    "            cv2.imwrite(output_path, left_marked)\n",
    "            print(f\"Saved both hands through LEFT-only landmark image: {output_path}\")\n",
    "        elif right_points == 42:\n",
    "            cv2.imwrite(output_path, right_marked)\n",
    "            print(f\"Saved both hands through RIGHT-only landmark image: {output_path}\")\n",
    "        else:\n",
    "            print(f\"Skipped {base} due to incomplete landmarks (left: {left_points}, right: {right_points})\")\n",
    "    elif has_left:\n",
    "        left_img = cv2.imread(left_path)\n",
    "        left_marked, left_points = draw_landmarks_on_image(left_img)\n",
    "        if left_points == 42:\n",
    "            cv2.imwrite(output_path, left_marked)\n",
    "            print(f\"Saved both hands through LEFT-only landmark image: {output_path}\")\n",
    "        else:\n",
    "            print(f\"Skipped {base} due to invalid LEFT-only landmarks ({left_points})\")\n",
    "    elif has_right:\n",
    "        right_img = cv2.imread(right_path)\n",
    "        right_marked, right_points = draw_landmarks_on_image(right_img)\n",
    "        if right_points == 42:\n",
    "            cv2.imwrite(output_path, right_marked)\n",
    "            print(f\"Saved both hands through RIGHT-only landmark image: {output_path}\")\n",
    "        else:\n",
    "            print(f\"Skipped {base} due to invalid RIGHT-only landmarks ({right_points})\")\n",
    "    elif has_base:\n",
    "        combined_img = cv2.imread(both_path)\n",
    "        both_marked, both_points = draw_landmarks_on_image(combined_img)\n",
    "        if both_points == 42:\n",
    "            cv2.imwrite(output_path, both_marked)\n",
    "            print(f\"Saved both hands through RIGHT-only landmark image: {output_path}\")\n",
    "        else:\n",
    "            print(f\"Skipped {base} due to invalid RIGHT-only landmarks ({both_points})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define single joint classifier\n",
    "class SingleJointInflammationClassifier(nn.Module):\n",
    "    def __init__(self, num_landmarks=16):\n",
    "        super().__init__()\n",
    "        self.cnn = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.cnn.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048 + 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image_input, landmark_input):\n",
    "        x_img = self.cnn(image_input)\n",
    "        x_landmark = self.mlp(landmark_input)\n",
    "        x = torch.cat([x_img, x_landmark], dim=1)\n",
    "        return self.classifier(x).squeeze(1)\n",
    "\n",
    "\n",
    "# Define dataset for single joint\n",
    "class SingleJointDataset(Dataset):\n",
    "    def __init__(self, image_paths, landmark_data, joint_labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.landmark_data = landmark_data\n",
    "        self.joint_labels = joint_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.from_numpy(np.load(self.image_paths[idx])).float()\n",
    "        # Normalize per channel\n",
    "        for c in range(image.shape[0]):\n",
    "            min_val = image[c].min()\n",
    "            max_val = image[c].max()\n",
    "            image[c] = (image[c] - min_val) / (max_val - min_val + 1e-6)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        landmarks = torch.tensor(self.landmark_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.joint_labels[idx], dtype=torch.float32)\n",
    "        return image, landmarks, label\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_single_joint(model, dataloader, optimizer, criterion, device, log_fn=print):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for images, landmarks, labels in dataloader:\n",
    "        images, landmarks, labels = images.to(device), landmarks.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, landmarks)\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1])\n",
    "    log_fn(\"Confusion Matrix:\\n\" + str(cm))\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_single_joint(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, landmarks, labels in dataloader:\n",
    "            images, landmarks, labels = images.to(device), landmarks.to(device), labels.to(device)\n",
    "            outputs = model(images, landmarks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return running_loss / len(dataloader), accuracy\n",
    "\n",
    "project_dir = \"C:/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "image_dir = os.path.join(project_dir, \"Data\", \"4_channel_images\")\n",
    "image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith(\".npy\")]\n",
    "\n",
    "processed_landmarks = np.load(os.path.join(project_dir, \"landmarks_optical\", \"processed_optical_landmarks\"))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# labels: shape (365, 32)\n",
    "num_pos = labels.sum(axis=0)  # shape (32,)\n",
    "num_neg = labels.shape[0] - num_pos  # shape (32,)\n",
    "\n",
    "# Avoid division by zero\n",
    "pos_weight = num_neg / (num_pos + 1e-5)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "pos_weight_tensor = torch.tensor(pos_weight, dtype=torch.float32)\n",
    "\n",
    "model_dir = os.path.join(project_dir, \"trained_models\")\n",
    "log_dir = os.path.join(project_dir, \"training_logs\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "for joint_idx in range(32):\n",
    "    model_path = os.path.join(model_dir, f\"joint_{joint_idx}_resnet50.pt\")\n",
    "    log_path = os.path.join(log_dir, f\"joint_{joint_idx}_training_log.txt\")\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Model for joint {joint_idx} already exists. Skipping training.\")\n",
    "        continue\n",
    "\n",
    "    with open(log_path, \"w\") as log_file:\n",
    "        def log(s):\n",
    "            print(s)\n",
    "            log_file.write(s + \"\\n\")\n",
    "\n",
    "        log(f\"\\n=== Training model for Joint {joint_idx} ===\")\n",
    "\n",
    "        # Prepare data\n",
    "        joint_landmarks = [lm[joint_idx] for lm in processed_landmarks]\n",
    "        joint_labels = [label[joint_idx] for label in labels]\n",
    "        dataset = SingleJointDataset(image_paths, joint_landmarks, joint_labels)\n",
    "\n",
    "        train_size = int(0.7 * len(dataset))\n",
    "        val_size = int(0.2 * len(dataset))\n",
    "        test_size = len(dataset) - train_size - val_size\n",
    "        train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "        model = SingleJointInflammationClassifier().to(device)\n",
    "        train_labels = [label.item() for _, _, label in train_dataset]\n",
    "        pos = sum(train_labels)\n",
    "        neg = len(train_labels) - pos\n",
    "        pos_weight = neg / (pos + 1e-5)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, dtype=torch.float32).to(device))\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(60):\n",
    "            train_loss = train_single_joint(model, train_loader, optimizer, criterion, device)\n",
    "            val_loss, val_acc = evaluate_single_joint(model, val_loader, criterion, device)\n",
    "            log(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}\")\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "        test_loss, test_acc = evaluate_single_joint(model, test_loader, criterion, device)\n",
    "        log(f\"Joint {joint_idx} - Test Loss={test_loss:.4f}, Test Accuracy={test_acc:.2f}\")\n",
    "        log(f\"Labels range: {np.min(joint_labels)}, {np.max(joint_labels)}\")\n",
    "\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        log(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register optical to thermal using landmarks and scaling that was tested to make sure it is consistent throughout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_hand_from_thermal(thermal_image, optical_segmented_image):\n",
    "    # Create a binary mask where the optical image is not black\n",
    "    # (any channel > 0 implies it's a hand pixel)\n",
    "    hand_mask = np.any(optical_segmented_image != 0, axis=-1).astype(np.uint8)  # shape: (H, W)\n",
    "\n",
    "    # Expand mask to 3 channels to match thermal image shape\n",
    "    #hand_mask_3ch = np.repeat(hand_mask[:, :, np.newaxis], 3, axis=2)  # shape: (H, W, 3)\n",
    "\n",
    "    # Apply mask to thermal image\n",
    "    masked_thermal = cv2.bitwise_and(thermal_image, thermal_image, mask=hand_mask)\n",
    "\n",
    "    return masked_thermal\n",
    "\n",
    "# === Paths ===\n",
    "project_dir = \"C:/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "\n",
    "# === Load MediaPipe model ===\n",
    "model_path = os.path.join(project_dir, \"backend\", \"models\", \"hand_landmarker.task\")\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    num_hands=2\n",
    ")\n",
    "\n",
    "landmarker = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "def detect_and_draw_landmarks(image, landmarker):\n",
    "    \"\"\"\n",
    "    Detect hand landmarks in an image and draw them on the image.\n",
    "\n",
    "    Args:\n",
    "        image (np.array): BGR image.\n",
    "        landmarker: Initialized MediaPipe HandLandmarker.\n",
    "\n",
    "    Returns:\n",
    "        image_with_landmarks (np.array): RGB image with landmarks drawn.\n",
    "        landmarks_list (list of tuples): List of (x, y) pixel coordinates for all detected landmarks.\n",
    "        total_landmarks (int): Total number of landmarks detected.\n",
    "    \"\"\"\n",
    "    # Convert BGR to RGB for MediaPipe processing\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)\n",
    "    result = landmarker.detect(mp_image)\n",
    "\n",
    "    landmarks_list = []\n",
    "    image_with_landmarks = rgb_image.copy()\n",
    "\n",
    "    if result.hand_landmarks:\n",
    "        for hand_landmarks in result.hand_landmarks:\n",
    "            for landmark in hand_landmarks:\n",
    "                x_px = int(landmark.x * image.shape[1])\n",
    "                y_px = int(landmark.y * image.shape[0])\n",
    "                landmarks_list.append((x_px, y_px))\n",
    "                cv2.circle(image_with_landmarks, (x_px, y_px), 3, (0, 255, 0), -1)\n",
    "\n",
    "    # Convert back to BGR before returning to keep consistent with OpenCV usage\n",
    "    image_with_landmarks = cv2.cvtColor(image_with_landmarks, cv2.COLOR_RGB2BGR)\n",
    "    return image_with_landmarks, landmarks_list, len(landmarks_list)\n",
    "\n",
    "def func_landmark(left_img=None, right_img=None, combined_img=None, \n",
    "                  has_left=False, has_right=False, has_both=False, landmarker=landmarker_two_hand):\n",
    "    \"\"\"\n",
    "    Process images to detect hand landmarks according to provided flags.\n",
    "\n",
    "    Returns landmarks list if valid, else None.\n",
    "    \"\"\"\n",
    "\n",
    "    if has_left and has_right:\n",
    "        left_marked, left_landmarks, left_count = detect_and_draw_landmarks(left_img, landmarker)\n",
    "        right_marked, right_landmarks, right_count = detect_and_draw_landmarks(right_img, landmarker)\n",
    "\n",
    "        combined = cv2.add(left_img, right_img)\n",
    "        combined_marked, combined_landmarks, combined_count = detect_and_draw_landmarks(combined, landmarker)\n",
    "\n",
    "        if left_count == 21 and right_count == 21 and combined_count == 42:\n",
    "            return combined_landmarks\n",
    "        elif combined_count != 42:\n",
    "            print(\"Error: Combined landmarks count not equal 42\")\n",
    "            return None\n",
    "        elif left_count == 42:\n",
    "            return left_landmarks\n",
    "        elif right_count == 42:\n",
    "            return right_landmarks\n",
    "        else:\n",
    "            print(f\"Skipped: incomplete landmarks (left: {left_count}, right: {right_count})\")\n",
    "            return None\n",
    "\n",
    "    elif has_left and left_img is not None:\n",
    "        marked, landmarks, count = detect_and_draw_landmarks(left_img, landmarker)\n",
    "        if count in [21, 42]:\n",
    "            return landmarks\n",
    "        print(f\"Skipped: invalid left-only landmarks count: {count}\")\n",
    "        return None\n",
    "\n",
    "    elif has_right and right_img is not None:\n",
    "        marked, landmarks, count = detect_and_draw_landmarks(right_img, landmarker)\n",
    "        if count == 42:\n",
    "            return landmarks\n",
    "        print(f\"Skipped: invalid right-only landmarks count: {count}\")\n",
    "        return None\n",
    "\n",
    "    elif has_both and combined_img is not None:\n",
    "        marked, landmarks, count = detect_and_draw_landmarks(combined_img, landmarker)\n",
    "        if count == 42:\n",
    "            return landmarks\n",
    "        print(f\"Skipped: invalid both-only landmarks count: {count}\")\n",
    "        return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def resize_scale(optical_image, scale_factor):\n",
    "    \"\"\"Resize optical image based on scale factor.\"\"\"\n",
    "    h, w = optical_image.shape[:2]\n",
    "    new_w, new_h = int(w * scale_factor), int(h * scale_factor)\n",
    "    return cv2.resize(optical_image, (new_w, new_h))\n",
    "\n",
    "def center_pad_image(smaller_img, target_shape):\n",
    "    small_h, small_w = smaller_img.shape[:2]\n",
    "    target_h, target_w = target_shape[:2]\n",
    "    \n",
    "    top = (target_h - small_h) // 2\n",
    "    bottom = target_h - small_h - top\n",
    "    left = (target_w - small_w) // 2\n",
    "    right = target_w - small_w - left\n",
    "\n",
    "    padded_img = cv2.copyMakeBorder(smaller_img, top, bottom, left, right,\n",
    "                                     borderType=cv2.BORDER_CONSTANT, value=0)\n",
    "    return padded_img\n",
    "\n",
    "def translate_image(image, x_shift=0, y_shift=0):\n",
    "    \"\"\"Translate image by x and y pixels. Positive y_shift moves down, negative moves up.\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    M = np.float32([[1, 0, x_shift], [0, 1, y_shift]])\n",
    "    translated = cv2.warpAffine(image, M, (width, height), borderValue=(0,0,0))\n",
    "    return translated\n",
    "\n",
    "# === Paths ===\n",
    "project_dir = \"/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "optical_folder = os.path.join(project_dir, \"optical_output_feathered_blur\")\n",
    "thermal_folder = os.path.join(project_dir, \"Data\", \"images\", \"inferno\")\n",
    "thermal_folder_landmark = os.path.join(project_dir, \"inferno_output\")\n",
    "output_folder = os.path.join(project_dir, \"masked_moprh_thermal_output_dynamic\")\n",
    "output_folder_shifted = os.path.join(project_dir, \"Data\", \"images\",\"shifted_inferno\")\n",
    "landmarks_output = os.path.join(project_dir, \"landmarks_optical\", \"all_optical_landmarks.npy\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "landmarks = []\n",
    "\n",
    "# === Loop through optical images and match with thermal ===\n",
    "for optical_name in os.listdir(optical_folder):\n",
    "    if not optical_name.endswith(\".jpg\"):\n",
    "        continue\n",
    "\n",
    "    # Extract base name: \"hands_optical_FLIR1231.jpg\" → \"FLIR1231.jpg\"\n",
    "    if not optical_name.startswith(\"hands_optical_\"):\n",
    "        continue\n",
    "    base_name = optical_name.replace(\"hands_optical_\", \"\")\n",
    "    segment_name_left = \"hand_left_\" + base_name\n",
    "    segment_name_right = \"hand_right_\" + base_name\n",
    "    morph_name = \"clean_\" + optical_name\n",
    "\n",
    "    thermal_path = os.path.join(thermal_folder, base_name)\n",
    "    thermal_segment_path_left = os.path.join(thermal_folder_landmark, segment_name_left)\n",
    "    thermal_segment_path_right = os.path.join(thermal_folder_landmark, segment_name_right)\n",
    "    optical_path = os.path.join(optical_folder, optical_name)\n",
    "\n",
    "    # If we don't have left we also don't have right because of the way they are saved\n",
    "    if not os.path.exists(thermal_path) or not os.path.exists(thermal_segment_path_left):\n",
    "        print(f\"Skipping {base_name}: thermal image not found.\")\n",
    "        continue\n",
    "\n",
    "    # Load images\n",
    "    thermal_image = cv2.imread(thermal_path)\n",
    "    thermal_segment_image_left = cv2.imread(thermal_segment_path_left)\n",
    "    thermal_segment_image_right = cv2.imread(thermal_segment_path_right)\n",
    "    optical_segmented = cv2.imread(optical_path)\n",
    "\n",
    "    scale = 1.41\n",
    "    # Apply resizing\n",
    "    optical_scaled = resize_scale(optical_segmented, scale)\n",
    "    #optical_scaled = cv2.cvtColor(optical_scaled, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if thermal_image.shape[:2] != (240, 320):\n",
    "        thermal_image = cv2.resize(thermal_image, (320, 240), interpolation=cv2.INTER_AREA)\n",
    "        thermal_segment_image_left = cv2.resize(thermal_segment_image_left, (320, 240), interpolation=cv2.INTER_AREA)\n",
    "        thermal_segment_image_right = cv2.resize(thermal_segment_image_right, (320, 240), interpolation=cv2.INTER_AREA)\n",
    "    #thermal_image = cv2.cvtColor(thermal_image, cv2.COLOR_BGR2RGB)\n",
    "    thermal_padded = center_pad_image(thermal_image, optical_scaled.shape)\n",
    "    thermal_segment_padded_left = center_pad_image(thermal_segment_image_left, optical_scaled.shape)\n",
    "    thermal_segment_padded_right = center_pad_image(thermal_segment_image_right, optical_scaled.shape)\n",
    "    \n",
    "    thermal_padded = cv2.resize(thermal_padded, (320, 240), interpolation=cv2.INTER_AREA)\n",
    "    thermal_segment_padded_left = cv2.resize(thermal_segment_padded_left, (320, 240), interpolation=cv2.INTER_AREA)\n",
    "    thermal_segment_padded_right = cv2.resize(thermal_segment_padded_right, (320, 240), interpolation=cv2.INTER_AREA)\n",
    "    optical_scaled = cv2.resize(optical_scaled, (320, 240), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    thermal_landmarks_left = func_landmark(thermal_segment_padded_left, \"\", \"\", True, False, False)\n",
    "    thermal_landmarks_right = func_landmark(\"\", thermal_landmarks_right, \"\", True, False, False)\n",
    "    optical_landmarks = func_landmark(\"\", \"\", optical_scaled, False, False, True)\n",
    "    # No optical or thermal landmarks detected, skip\n",
    "    if ((thermal_landmarks_left == None and thermal_landmarks_right == None) or optical_landmarks == None):\n",
    "        continue\n",
    "    optical_point_left = 9\n",
    "    optical_point_right = 30\n",
    "    if (optical_landmarks[9][0] > (optical_scaled.shape[1] // 2)):\n",
    "        optical_point_left = optical_point_left + 21\n",
    "        optical_point_right = 9\n",
    "    # No left landmarks, use the right\n",
    "    if thermal_landmarks_left == None:\n",
    "        x_shift = optical_landmarks[optical_point_right][0] - thermal_landmarks_right[9][0]\n",
    "        y_shift = optical_landmarks[optical_point_right][1] - thermal_landmarks_right[9][1]\n",
    "    # Use left landmarks\n",
    "    else :\n",
    "        x_shift = optical_landmarks[optical_point_left][0] - thermal_landmarks_left[9][0]\n",
    "        y_shift = optical_landmarks[optical_point_left][1] - thermal_landmarks_left[9][1]\n",
    "\n",
    "    shifted_thermal = translate_image(thermal_padded, y_shift=y_shift, x_shift=x_shift)\n",
    "\n",
    "    # Apply mask\n",
    "    masked_thermal = extract_hand_from_thermal(shifted_thermal, optical_scaled)\n",
    "\n",
    "    # Save\n",
    "    output_path = os.path.join(output_folder, f\"masked_{base_name}\")\n",
    "    cv2.imwrite(output_path, masked_thermal)\n",
    "    cv2.imwrite(output_folder_shifted, shifted_thermal)\n",
    "    landmarks.append(optical_landmarks)\n",
    "# Landmarks array save    \n",
    "landmarks_np = np.array(landmarks)\n",
    "np.save(landmarks_output, landmarks_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "project_dir = \"C:/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "landmark_dir = os.path.join(project_dir, \"inferno_landmarks\")\n",
    "\n",
    "# === Load MediaPipe model ===\n",
    "model_path = os.path.join(project_dir, \"models\", \"hand_landmarker.task\")\n",
    "\n",
    "# List to collect filenames of images with a full hand detection\n",
    "valid_images = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(landmark_dir):\n",
    "    # Only process files that start with \"landmarks_\" (to match the expected files)\n",
    "    if not filename.startswith(\"landmarks_\"):\n",
    "        continue\n",
    "    file_path = os.path.join(landmark_dir, filename)\n",
    "\n",
    "    # Attempt to load landmark coordinates from a corresponding JSON file (if it exists)\n",
    "    coords = None\n",
    "    base_name, ext = os.path.splitext(file_path)\n",
    "    json_path = base_name + \".json\"  # e.g., \"inferno_landmarks/landmarks_FLIR1231_left_hand.json\"\n",
    "    if os.path.exists(json_path):\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            data = None\n",
    "\n",
    "        if data is not None:\n",
    "            # Determine how the coordinates are stored and extract them\n",
    "            if isinstance(data, list):\n",
    "                # If data is a list, it could be:\n",
    "                # - A list of 21 coordinate points (each point might be [x,y] or a dict with coords).\n",
    "                # - A list of multiple hand landmark sets (e.g., [hand1_points, hand2_points]).\n",
    "                if len(data) == 21:\n",
    "                    coords = data  # 21 landmarks directly in a list\n",
    "                elif len(data) > 0 and isinstance(data[0], list) and len(data[0]) == 21:\n",
    "                    coords = data[0]  # first hand's landmarks from a list of hand landmarks\n",
    "                elif len(data) == 21 and isinstance(data[0], dict):\n",
    "                    coords = data  # list of 21 dicts (each with x,y maybe)\n",
    "            elif isinstance(data, dict):\n",
    "                # If data is a dict, look for a key that contains landmark list\n",
    "                for key in ['landmarks', 'hand_landmarks', 'points', 'coordinates']:\n",
    "                    if key in data and isinstance(data[key], list):\n",
    "                        val = data[key]\n",
    "                        if len(val) == 21:\n",
    "                            coords = val  # found 21 landmarks in the list\n",
    "                        elif len(val) > 0 and isinstance(val[0], list) and len(val[0]) == 21:\n",
    "                            coords = val[0]  # first set of 21 landmarks\n",
    "                        elif len(val) == 21 and isinstance(val[0], dict):\n",
    "                            coords = val  # list of 21 coordinate dicts\n",
    "                        # If found the right key, no need to check other keys\n",
    "                        if coords is not None:\n",
    "                            break\n",
    "\n",
    "    # Determine if this image has a full hand detection\n",
    "    full_hand_detected = False\n",
    "\n",
    "    if coords is not None:\n",
    "        # If we have coordinates loaded, check if they represent 21 landmarks\n",
    "        if isinstance(coords, list) and len(coords) == 21:\n",
    "            full_hand_detected = True\n",
    "    else:\n",
    "        # No coordinate data available; fall back to analyzing the image for green landmarks.\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is not None:\n",
    "            # Define the color range for the green landmark dots (in BGR color space)\n",
    "            lower_green = np.array([0, 250, 0], dtype=np.uint8)  # allow slight variation in green\n",
    "            upper_green = np.array([0, 255, 0], dtype=np.uint8)\n",
    "            mask = cv2.inRange(image, lower_green, upper_green)\n",
    "            # Check if there are any green pixels in the mask\n",
    "            if np.count_nonzero(mask) > 0:\n",
    "                full_hand_detected = True\n",
    "\n",
    "    # If a full hand (21 landmarks) is detected, add the filename to the valid list\n",
    "    if full_hand_detected:\n",
    "        valid_images.append(filename)\n",
    "\n",
    "# Print the filenames of images with valid full-hand detections\n",
    "# print(\"Images with full 21-landmark detections:\")\n",
    "# for fname in valid_images:\n",
    "#     print(fname)\n",
    "\n",
    "with open(os.path.join(project_dir, \"valid_thermal_images.txt\"), \"w\") as f:\n",
    "    for fname in valid_images:\n",
    "        f.write(fname + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown We implemented some functions to visualize the hand landmark detection results. <br/> Run the following cell to activate the functions.\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  hand_landmarks_list = detection_result.hand_landmarks\n",
    "  handedness_list = detection_result.handedness\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected hands to visualize.\n",
    "  for idx in range(len(hand_landmarks_list)):\n",
    "    hand_landmarks = hand_landmarks_list[idx]\n",
    "    handedness = handedness_list[idx]\n",
    "\n",
    "    # Draw the hand landmarks.\n",
    "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    hand_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      hand_landmarks_proto,\n",
    "      solutions.hands.HAND_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "      solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Get the top left corner of the detected hand's bounding box.\n",
    "    height, width, _ = annotated_image.shape\n",
    "    x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "    y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "    text_x = int(min(x_coordinates) * width)\n",
    "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "    # Draw handedness (left or right hand) on the image.\n",
    "    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
    "\n",
    "# === Enhance contrast ===\n",
    "def enhance_contrast(image):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "# === Improved feathering ===\n",
    "def feather_edges(image, mask, feather_amount=5):\n",
    "    mask = mask.astype(np.float32)\n",
    "\n",
    "    # Dilate mask to preserve fingers and edges\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    mask_dilated = cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "    # Blur only the border by subtracting original from dilated\n",
    "    edge = mask_dilated - mask\n",
    "    edge_blur = cv2.GaussianBlur(edge, (0, 0), feather_amount)\n",
    "\n",
    "    # Create smooth transition alpha mask\n",
    "    soft_mask = np.clip(mask + edge_blur, 0, 1)\n",
    "    alpha = np.stack([soft_mask] * 3, axis=-1)\n",
    "\n",
    "    blended = (image * alpha).astype(np.uint8)\n",
    "    return blended\n",
    "\n",
    "# === Settings ===\n",
    "project_dir = \"/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "images_base_dir = os.path.join(project_dir, \"data\", \"images\")\n",
    "desired_size = (320, 240)\n",
    "w, h = desired_size[0], desired_size[1]\n",
    "min_area = 1000\n",
    "\n",
    "# === Load SAM ===\n",
    "sam = sam_model_registry[\"vit_h\"](\n",
    "    checkpoint=os.path.join(project_dir, \"models\", \"sam_vit_h_4b8939.pth\")\n",
    ").to(\"cuda\")\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "# === Loop through subfolders ===\n",
    "for colormap_folder in os.listdir(images_base_dir):\n",
    "    if colormap_folder == \"optical\":\n",
    "        folder_path = os.path.join(images_base_dir, colormap_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        output_dir = os.path.join(project_dir, f\"{colormap_folder}_output6\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"\\n📁 Processing folder: {colormap_folder}\")\n",
    "\n",
    "        for file_path in glob.glob(f\"{folder_path}/*.jpg\"):\n",
    "            filename = os.path.basename(file_path)\n",
    "            print(f\"\\n🔄 Processing image: {filename}\")\n",
    "\n",
    "            image = cv2.imread(file_path)\n",
    "            # Moved this line to save part\n",
    "            #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if image.shape[1] != desired_size[0] or image.shape[0] != desired_size[1]:\n",
    "                image = cv2.resize(image, desired_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            image = enhance_contrast(image)\n",
    "\n",
    "            masks = mask_generator.generate(image)\n",
    "\n",
    "            image_area = h * w\n",
    "            valid_masks = []\n",
    "            used_indexes = set()\n",
    "            width_upper, width_lower, area_lower, area_higher = 0.5, 0.15, 0.1, 0.33\n",
    "\n",
    "            # Filter masks based on area ratio\n",
    "            for i, m in enumerate(masks):\n",
    "                if i in used_indexes:\n",
    "                    continue  # Skip already used masks\n",
    "                mask = m[\"segmentation\"].astype(np.uint8)\n",
    "                area = cv2.countNonZero(mask)\n",
    "                area_ratio = area / image_area\n",
    "\n",
    "                # === Width check ===\n",
    "                x, y, mask_w, mask_h = cv2.boundingRect(mask)\n",
    "                width_ratio = mask_w / w\n",
    "                height_ratio = mask_h / h\n",
    "                if width_ratio > width_upper or width_ratio < width_lower or height_ratio > 0.8:\n",
    "                    continue\n",
    "\n",
    "                if area_lower <= area_ratio <= area_higher:\n",
    "                    valid_masks.append((i, mask))\n",
    "                    used_indexes.add(i)\n",
    "                    if len(valid_masks) == 2:\n",
    "                        break\n",
    "\n",
    "            if len(valid_masks) < 2:\n",
    "                print(\"↪️ Trying split-image fallback.\")\n",
    "\n",
    "                mid_x = w // 2\n",
    "                left_half = image[:, :mid_x]\n",
    "                right_half = image[:, mid_x:]\n",
    "\n",
    "                valid_masks = []\n",
    "\n",
    "                for half_image, offset_x, label in [(left_half, 0, \"left\"), (right_half, mid_x, \"right\")]:\n",
    "                    half_masks = mask_generator.generate(half_image)\n",
    "                    for i, m in enumerate(half_masks):\n",
    "                        if i in used_indexes:\n",
    "                            continue  # Skip already used masks\n",
    "                        mask = m[\"segmentation\"].astype(np.uint8)\n",
    "                        area = cv2.countNonZero(mask)\n",
    "                        area_ratio = area / (h * (w // 2))\n",
    "\n",
    "                        x, y, mask_w, mask_h = cv2.boundingRect(mask)\n",
    "                        width_ratio = mask_w / (w // 2)\n",
    "                        height_ratio = mask_h / h\n",
    "\n",
    "                        if not ((width_lower / 2) <= width_ratio <= width_upper and height_ratio <= 0.8):\n",
    "                            continue\n",
    "                        if 0.06 <= area_ratio <= 0.33:\n",
    "                            # Shift mask to full image size\n",
    "                            full_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                            full_mask[:, offset_x:offset_x + (w // 2)] = mask\n",
    "                            valid_masks.append((i, full_mask))\n",
    "                            print(f\"Found {label} hand in split image.\")\n",
    "                            break  # Only one hand needed per side\n",
    "\n",
    "            # Save the two valid masks\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            for i, (idx, mask) in enumerate(valid_masks):\n",
    "                feathered_hand = feather_edges(image, mask, feather_amount=5)\n",
    "                side = \"left\" if i == 0 else \"right\"\n",
    "                output_path = os.path.join(output_dir, f\"{base_name}_{side}_hand.jpg\")\n",
    "                cv2.imwrite(output_path, cv2.cvtColor(feathered_hand, cv2.COLOR_RGB2BGR))\n",
    "                print(f\"Saved {side} hand (mask {idx}) to: {output_path}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "\n",
    "# === Load SAM ===\n",
    "project_dir = \"/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "sam = sam_model_registry[\"vit_h\"](\n",
    "    checkpoint=os.path.join(project_dir, \"backend\", \"models\", \"sam_vit_h_4b8939.pth\")\n",
    ").to(\"cuda\")\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "# === Load MediaPipe model ===\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options_two_hand = vision.HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=\n",
    "                             os.path.join(project_dir, \"backend\", \n",
    "                                          \"models\", \"hand_landmarker.task\")),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    num_hands=2\n",
    ")\n",
    "\n",
    "landmarker_two_hand = vision.HandLandmarker.create_from_options(options_two_hand)\n",
    "\n",
    "options_one_hand = vision.HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=\n",
    "                             os.path.join(project_dir, \"backend\", \n",
    "                                          \"models\", \"hand_landmarker.task\")),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    num_hands=1\n",
    ")\n",
    "\n",
    "landmarker_one_hand = vision.HandLandmarker.create_from_options(options_one_hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# === Enhance contrast ===\n",
    "def enhance_contrast(image):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "# === Improved feathering ===\n",
    "def feather_edges(image, mask, feather_amount=5):\n",
    "    mask = mask.astype(np.float32)\n",
    "\n",
    "    # Dilate mask to preserve fingers and edges\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    mask_dilated = cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "    # Blur only the border by subtracting original from dilated\n",
    "    edge = mask_dilated - mask\n",
    "    edge_blur = cv2.GaussianBlur(edge, (0, 0), feather_amount)\n",
    "\n",
    "    # Create smooth transition alpha mask\n",
    "    soft_mask = np.clip(mask + edge_blur, 0, 1)\n",
    "    alpha = np.stack([soft_mask] * 3, axis=-1)\n",
    "\n",
    "    blended = (image * alpha).astype(np.uint8)\n",
    "    return blended\n",
    "\n",
    "def count_hand_landmarks(image):\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "    result = landmarker.detect(mp_image)\n",
    "    return sum(len(hand) for hand in result.hand_landmarks) if result.hand_landmarks else 0\n",
    "\n",
    "\n",
    "def average_min_distance_to_corners(mask):\n",
    "    h, w = mask.shape\n",
    "\n",
    "    # === Check center color distance to undesired color ===\n",
    "    center_pixel = mask[h // 2, w // 2].astype(np.float32)\n",
    "    undesired_color = np.array([181, 192, 219], dtype=np.float32)\n",
    "\n",
    "    # Euclidean color distance in RGB space\n",
    "    color_dist = np.linalg.norm(center_pixel - undesired_color)\n",
    "\n",
    "    # Normalize to [0, 1] range (max distance in RGB is sqrt(3 * 255^2))\n",
    "    max_rgb_dist = np.sqrt(3 * 255**2)\n",
    "    color_similarity_penalty = color_dist / max_rgb_dist  # 1 = far, 0 = exact match\n",
    "\n",
    "    corners = np.array([\n",
    "        [0, 0],              # top-left\n",
    "        [0, w - 1],          # top-right\n",
    "        [h // 2, 0],         # middle-left\n",
    "        [h // 2, w - 1],     # middle-right\n",
    "        [0, w // 2],         # top-middle\n",
    "        [0, w // 2]          # count center twice\n",
    "    ])\n",
    "\n",
    "    ys, xs = np.nonzero(mask)\n",
    "    points = np.stack([ys, xs], axis=1)\n",
    "\n",
    "    if len(points) == 0:\n",
    "        return np.inf  # empty mask\n",
    "\n",
    "    # Base distance score\n",
    "    min_dists = [np.min(np.linalg.norm(points - corner, axis=1)) for corner in corners]\n",
    "    score = np.sum(min_dists)\n",
    "\n",
    "    # === Center Proximity Score ===\n",
    "    mask_center = np.mean(points, axis=0)  # (y, x)\n",
    "    center = np.array([h // 2, w // 2])\n",
    "    avg_center_dist = np.linalg.norm(mask_center - center)\n",
    "\n",
    "    # Max possible distance = image diagonal (for normalization)\n",
    "    max_dist = np.linalg.norm([h / 2, w / 2])\n",
    "    center_score = (1 - (avg_center_dist / max_dist)) * score  # closer = higher score\n",
    "    score += center_score\n",
    "\n",
    "    # === Punish if mask touches any edge ===\n",
    "    if np.any(mask[0, :]):            # top edge\n",
    "        score *= 0.5\n",
    "    if np.any(mask[0:h//2, 0]):            # left edge\n",
    "        score *= 0.5\n",
    "    if np.any(mask[0:h//2, w - 1]):        # right edge\n",
    "        score *= 0.5\n",
    "\n",
    "    # === Apply color similarity penalty ===\n",
    "    score *= color_similarity_penalty  # closer to undesired color -> score is down-weighted\n",
    "\n",
    "    # === Apply area size penalty\n",
    "    mask_area_ratio = len(points) / (h * w)\n",
    "\n",
    "    score *= np.log1p(mask_area_ratio * 100)  # log1p = log(1 + x)\n",
    "\n",
    "    return abs(score)\n",
    "\n",
    "# Filter masks based on area ratio\n",
    "def filterMasks(width_upper, width_lower, area_lower, area_higher, original_image, masks):\n",
    "    valid_masks, valid_dist = [], []\n",
    "    for i, m in enumerate(masks):\n",
    "        mask = m[\"segmentation\"].astype(np.uint8)\n",
    "        area = cv2.countNonZero(mask)\n",
    "        area_ratio = area / image_area\n",
    "\n",
    "        masked_image = feather_edges(original_image, mask, feather_amount=5)\n",
    "        num_landmarks = count_hand_landmarks(masked_image)\n",
    "\n",
    "        if (num_landmarks != 42):\n",
    "            continue\n",
    "\n",
    "        # === Width check ===\n",
    "        x, y, mask_w, mask_h = cv2.boundingRect(mask)\n",
    "        width_ratio = mask_w / w\n",
    "        height_ratio = mask_h / h\n",
    "        if width_ratio > width_upper or width_ratio < width_lower or height_ratio > 0.9:\n",
    "            continue\n",
    "        \n",
    "        if area_lower <= area_ratio <= area_higher:\n",
    "            valid_masks.append((i, mask))\n",
    "            valid_dist.append(average_min_distance_to_corners(mask))\n",
    "    return valid_masks, valid_dist\n",
    "\n",
    "# === Settings ===\n",
    "project_dir = \"/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "images_base_dir = os.path.join(project_dir, \"data\", \"images\")\n",
    "desired_size = (320, 240)\n",
    "w, h = desired_size[0], desired_size[1]\n",
    "min_area = 1000\n",
    "\n",
    "# === Loop through subfolders ===\n",
    "for colormap_folder in os.listdir(images_base_dir):\n",
    "    if colormap_folder == \"optical\":\n",
    "        folder_path = os.path.join(images_base_dir, colormap_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        output_dir = os.path.join(project_dir, f\"{colormap_folder}_output_white_punished\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"\\n📁 Processing folder: {colormap_folder}\")\n",
    "\n",
    "        for file_path in glob.glob(f\"{folder_path}/*.jpg\"):\n",
    "            filename = os.path.basename(file_path)\n",
    "            print(f\"\\n🔄 Processing image: {filename}\")\n",
    "            image = cv2.imread(file_path)\n",
    "\n",
    "            if image.shape[1] != desired_size[0] or image.shape[0] != desired_size[1]:\n",
    "                image = cv2.resize(image, desired_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            #image = enhance_contrast(image)\n",
    "\n",
    "            masks = mask_generator.generate(image)\n",
    "\n",
    "            image_area = h * w\n",
    "            valid_masks = []\n",
    "            valid_dist = []\n",
    "            width_upper, width_lower, area_lower, area_higher = 0.95, 0.1, 0.08, 0.82\n",
    "            valid_masks, valid_dist = filterMasks(width_upper,\n",
    "                                                  width_lower,\n",
    "                                                  area_lower,\n",
    "                                                  area_higher,\n",
    "                                                  image,\n",
    "                                                  masks)\n",
    "            if not valid_masks:\n",
    "                print(\"couldn't find masks, trying with lower width limit again\")\n",
    "                valid_masks, valid_dist = filterMasks(width_upper + 0.05, width_lower - 0.05,\n",
    "                            area_lower - 0.03, area_higher, image, masks)\n",
    "            save_mask = []\n",
    "            max_dist = -1\n",
    "            best_index = -1\n",
    "            for idx in range(len(valid_dist)):\n",
    "                if valid_dist[idx] > max_dist:\n",
    "                    max_dist = valid_dist[idx]\n",
    "                    best_index = idx\n",
    "            if best_index != -1:\n",
    "                save_mask.append(valid_masks[best_index])\n",
    "            # Save the two valid masks\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            for i, (idx, mask) in enumerate(save_mask):\n",
    "                feathered_hand = feather_edges(image, mask, feather_amount=5)\n",
    "                output_path = os.path.join(output_dir, f\"hands_{base_name}.jpg\")\n",
    "                cv2.imwrite(output_path, cv2.cvtColor(feathered_hand, cv2.COLOR_BGR2RGB))\n",
    "                print(f\"Saved hands (mask {idx}) to: {output_path}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best optical segmentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# === Enhance contrast ===\n",
    "def enhance_contrast(image):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "# === Improved feathering ===\n",
    "def feather_edges(image, mask, feather_amount=5):\n",
    "    mask = mask.astype(np.float32)\n",
    "\n",
    "    # Dilate mask to preserve fingers and edges\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    mask_dilated = cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "    # Blur only the border by subtracting original from dilated\n",
    "    edge = mask_dilated - mask\n",
    "    edge_blur = cv2.GaussianBlur(edge, (0, 0), feather_amount)\n",
    "\n",
    "    # Create smooth transition alpha mask\n",
    "    soft_mask = np.clip(mask + edge_blur, 0, 1)\n",
    "    alpha = np.stack([soft_mask] * 3, axis=-1)\n",
    "\n",
    "    blended = (image * alpha).astype(np.uint8)\n",
    "    return blended\n",
    "\n",
    "def count_hand_landmarks(landmarker, image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "    result = landmarker.detect(mp_image)\n",
    "    return sum(len(hand) for hand in result.hand_landmarks) if result.hand_landmarks else 0\n",
    "\n",
    "def average_min_distance_to_corners(mask):\n",
    "    h, w = mask.shape\n",
    "\n",
    "    corners = np.array([\n",
    "        [0, 0],              # top-left\n",
    "        [0, w - 1],          # top-right\n",
    "        [h // 2, 0],         # middle-left\n",
    "        [h // 2, w - 1],     # middle-right\n",
    "        [0, w // 2],         # top-middle\n",
    "        [0, w // 2]          # count center twice\n",
    "    ])\n",
    "\n",
    "    ys, xs = np.nonzero(mask)\n",
    "    points = np.stack([ys, xs], axis=1)\n",
    "\n",
    "    if len(points) == 0:\n",
    "        return np.inf  # empty mask\n",
    "\n",
    "    # Base distance score\n",
    "    min_dists = [np.min(np.linalg.norm(points - corner, axis=1)) for corner in corners]\n",
    "    score = np.sum(min_dists)\n",
    "\n",
    "    # === Center Proximity Score ===\n",
    "    mask_center = np.mean(points, axis=0)  # (y, x)\n",
    "    center = np.array([h // 2, w // 2])\n",
    "    avg_center_dist = np.linalg.norm(mask_center - center)\n",
    "\n",
    "    # Max possible distance = image diagonal (for normalization)\n",
    "    max_dist = np.linalg.norm([h / 2, w / 2])\n",
    "    center_score = (1 + avg_center_dist / max_dist) * score / 2\n",
    "    score += center_score\n",
    "\n",
    "    # === Punish if mask touches any edge ===\n",
    "    if np.any(mask[0, :]):            # top edge\n",
    "        score *= 0.5\n",
    "    if np.any(mask[0:h//2, 0]):            # left edge\n",
    "        score *= 0.5\n",
    "    if np.any(mask[0:h//2, w - 1]):        # right edge\n",
    "        score *= 0.5\n",
    "\n",
    "    return abs(score)\n",
    "\n",
    "# Filter masks based on area ratio\n",
    "def filterMasks(width_upper, width_lower, area_lower, area_higher, original_image, masks, landmarker, num_hands):\n",
    "    valid_masks, valid_dist = [], []\n",
    "    for i, m in enumerate(masks):\n",
    "        mask = m[\"segmentation\"].astype(np.uint8)\n",
    "        area = cv2.countNonZero(mask)\n",
    "        area_ratio = area / image_area\n",
    "\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        masked_image = feather_edges(original_image, mask, feather_amount=5)\n",
    "        num_landmarks = count_hand_landmarks(landmarker, masked_image)\n",
    "\n",
    "        if (num_hands == 2 and num_landmarks != 42):\n",
    "            continue\n",
    "\n",
    "        if (num_hands == 1 and num_landmarks != 21):\n",
    "            continue\n",
    "\n",
    "        # === Width check ===\n",
    "        x, y, mask_w, mask_h = cv2.boundingRect(mask)\n",
    "        width_ratio = mask_w / w\n",
    "        height_ratio = mask_h / h\n",
    "        # if width_ratio > width_upper or width_ratio < width_lower or height_ratio > 0.9:\n",
    "        #     continue\n",
    "        \n",
    "        if area_lower <= area_ratio <= area_higher:\n",
    "            valid_masks.append((i, mask))\n",
    "            valid_dist.append(average_min_distance_to_corners(mask))\n",
    "    return valid_masks, valid_dist\n",
    "\n",
    "# === Settings ===\n",
    "project_dir = \"/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "images_base_dir = os.path.join(project_dir, \"data\", \"images\")\n",
    "desired_size = (320, 240)\n",
    "w, h = desired_size[0], desired_size[1]\n",
    "min_area = 1000\n",
    "\n",
    "# === Loop through subfolders ===\n",
    "for colormap_folder in os.listdir(images_base_dir):\n",
    "    if colormap_folder == \"optical\":\n",
    "        folder_path = os.path.join(images_base_dir, colormap_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        output_dir = os.path.join(project_dir, f\"{colormap_folder}_output_landmark_combined\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"\\n📁 Processing folder: {colormap_folder}\")\n",
    "\n",
    "        for file_path in glob.glob(f\"{folder_path}/*.jpg\"):\n",
    "            filename = os.path.basename(file_path)\n",
    "            print(f\"\\n🔄 Processing image: {filename}\")\n",
    "            image = cv2.imread(file_path)\n",
    "\n",
    "            if image.shape[1] != desired_size[0] or image.shape[0] != desired_size[1]:\n",
    "                image = cv2.resize(image, desired_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            #image = enhance_contrast(image)\n",
    "\n",
    "            masks = mask_generator.generate(image)\n",
    "\n",
    "            image_area = h * w\n",
    "            valid_masks = []\n",
    "            valid_dist = []\n",
    "            two_separate = False\n",
    "            width_upper, width_lower, area_lower, area_higher = 0.9, 0.2, 0.06, 0.82\n",
    "            valid_masks, valid_dist = filterMasks(width_upper,\n",
    "                                                  width_lower,\n",
    "                                                  area_lower,\n",
    "                                                  area_higher,\n",
    "                                                  image,\n",
    "                                                  masks,\n",
    "                                                  landmarker_two_hand,\n",
    "                                                  2)\n",
    "            if not valid_masks:\n",
    "                print(\"couldn't find masks, trying to find one hand at a time\")\n",
    "                valid_masks, valid_dist = filterMasks(width_upper + 0.05, \n",
    "                                                      width_lower - 0.05,\n",
    "                                                      area_lower - 0.03, \n",
    "                                                      area_higher + 0.08,\n",
    "                                                      image, \n",
    "                                                      masks, \n",
    "                                                      landmarker_one_hand,\n",
    "                                                      1)\n",
    "                two_separate = True\n",
    "            save_mask = []\n",
    "            if ( two_separate ):\n",
    "                scored_masks = sorted(zip(valid_dist, valid_masks), key=lambda x: x[0], reverse=True)\n",
    "                \n",
    "                if not scored_masks:\n",
    "                    continue\n",
    "                elif len(scored_masks) > 1:\n",
    "                    top_masks = scored_masks[:2]\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    feathered_hands = [\"feathered_hand_1\", \"feathered_hand_2\"]\n",
    "                    for i, (score, (idx, mask)) in enumerate(top_masks):\n",
    "                        feathered_hands[i] = feather_edges(image, mask, feather_amount=5)\n",
    "                    combined = cv2.add(feathered_hands[0], feathered_hands[1])\n",
    "                    base_name = os.path.splitext(filename)[0]\n",
    "                    output_path = os.path.join(output_dir, f\"hands_{base_name}.jpg\")\n",
    "                    # Check for 42 landmarks before saving\n",
    "                    num_landmarks = count_hand_landmarks(landmarker_two_hand, combined)\n",
    "                    if num_landmarks == 42:\n",
    "                        cv2.imwrite(output_path, cv2.cvtColor(combined, cv2.COLOR_BGR2RGB))\n",
    "                        print(f\"Saved hands (mask combined) to: {output_path}\")\n",
    "                    else:\n",
    "                        print(\"Skipped: Landmark count != 42 after combining masks.\")\n",
    "\n",
    "            else:\n",
    "                max_dist = -1\n",
    "                best_index = -1\n",
    "                for idx in range(len(valid_dist)):\n",
    "                    if valid_dist[idx] > max_dist:\n",
    "                        max_dist = valid_dist[idx]\n",
    "                        best_index = idx\n",
    "                if best_index != -1:\n",
    "                    save_mask.append(valid_masks[best_index])\n",
    "                # Save the two valid masks\n",
    "                base_name = os.path.splitext(filename)[0]\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                for i, (idx, mask) in enumerate(save_mask):\n",
    "                    feathered_hand = feather_edges(image, mask, feather_amount=5)\n",
    "                    output_path = os.path.join(output_dir, f\"hands_{base_name}.jpg\")\n",
    "                    num_landmarks = count_hand_landmarks(landmarker_two_hand, feathered_hand)\n",
    "                    if num_landmarks == 42:\n",
    "                        cv2.imwrite(output_path, cv2.cvtColor(feathered_hand, cv2.COLOR_BGR2RGB))\n",
    "                        print(f\"Saved hands (mask {idx}) to: {output_path}\")\n",
    "                    else:\n",
    "                        print(\"Skipped: Landmark count != 42 on selected mask.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "\n",
    "# === Enhance contrast ===\n",
    "def enhance_contrast(image):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "# === Improved feathering ===\n",
    "def feather_edges(image, mask, feather_amount=5):\n",
    "    mask = mask.astype(np.float32)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    mask_dilated = cv2.dilate(mask, kernel, iterations=1)\n",
    "    edge = mask_dilated - mask\n",
    "    edge_blur = cv2.GaussianBlur(edge, (0, 0), feather_amount)\n",
    "    soft_mask = np.clip(mask + edge_blur, 0, 1)\n",
    "    alpha = np.stack([soft_mask] * 3, axis=-1)\n",
    "    blended = (image * alpha).astype(np.uint8)\n",
    "    return blended\n",
    "\n",
    "# === Settings ===\n",
    "project_dir = \"/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "images_base_dir = os.path.join(project_dir, \"data\", \"images\")\n",
    "desired_size = (320, 240)\n",
    "w, h = desired_size\n",
    "\n",
    "# === Load SAM ===\n",
    "sam = sam_model_registry[\"vit_h\"](\n",
    "    checkpoint=os.path.join(project_dir, \"models\", \"sam_vit_h_4b8939.pth\")\n",
    ").to(\"cuda\")\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "# === Processing ===\n",
    "for colormap_folder in os.listdir(images_base_dir):\n",
    "    if colormap_folder == \"optical\":\n",
    "        folder_path = os.path.join(images_base_dir, colormap_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        output_dir = os.path.join(project_dir, f\"{colormap_folder}_output_final\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for file_path in glob.glob(f\"{folder_path}/*.jpg\"):\n",
    "            filename = os.path.basename(file_path)\n",
    "            image = cv2.imread(file_path)\n",
    "\n",
    "            if image.shape[1] != w or image.shape[0] != h:\n",
    "                image = cv2.resize(image, desired_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            enhanced_image = enhance_contrast(image_rgb)\n",
    "\n",
    "            masks = mask_generator.generate(enhanced_image)\n",
    "\n",
    "            valid_masks = []\n",
    "            for m in masks:\n",
    "                mask = m[\"segmentation\"].astype(np.uint8)\n",
    "                area_ratio = cv2.countNonZero(mask) / (h * w)\n",
    "\n",
    "                x, y, mask_w, mask_h = cv2.boundingRect(mask)\n",
    "                width_ratio = mask_w / w\n",
    "                height_ratio = mask_h / h\n",
    "                if 0.2 <= width_ratio <= 0.9 and height_ratio <= 0.9 and 0.15 <= area_ratio <= 0.8:\n",
    "                    valid_masks.append(mask)\n",
    "\n",
    "            image_center_x = w / 2\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "\n",
    "            for idx, mask in enumerate(valid_masks):\n",
    "                x, y, mask_w, mask_h = cv2.boundingRect(mask)\n",
    "                mask_center_x = x + mask_w / 2\n",
    "                side = \"left\" if mask_center_x < image_center_x else \"right\"\n",
    "\n",
    "                # Save feathered hand\n",
    "                feathered_hand = feather_edges(enhanced_image, mask, feather_amount=5)\n",
    "                output_hand_path = os.path.join(output_dir, f\"{base_name}_{side}_hand.jpg\")\n",
    "                cv2.imwrite(output_hand_path, cv2.cvtColor(feathered_hand, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "                # Save binary mask\n",
    "                binary_mask_output = (mask * 255).astype(np.uint8)\n",
    "                output_mask_path = os.path.join(output_dir, f\"{base_name}_{side}_mask.png\")\n",
    "                cv2.imwrite(output_mask_path, binary_mask_output)\n",
    "\n",
    "                print(f\"Saved {side} hand and mask to: {output_hand_path}, {output_mask_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "\n",
    "# === Enhance contrast ===\n",
    "def enhance_contrast(image):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "# === Improved feathering ===\n",
    "def feather_edges(image, mask, feather_amount=5):\n",
    "    mask = mask.astype(np.float32)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    mask_dilated = cv2.dilate(mask, kernel, iterations=1)\n",
    "    edge = mask_dilated - mask\n",
    "    edge_blur = cv2.GaussianBlur(edge, (0, 0), feather_amount)\n",
    "    soft_mask = np.clip(mask + edge_blur, 0, 1)\n",
    "    alpha = np.stack([soft_mask] * 3, axis=-1)\n",
    "    blended = (image * alpha).astype(np.uint8)\n",
    "    return blended\n",
    "\n",
    "# === Settings ===\n",
    "project_dir = \"/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "images_base_dir = os.path.join(project_dir, \"data\", \"images\")\n",
    "desired_size = (320, 240)\n",
    "w, h = desired_size\n",
    "\n",
    "# === Load SAM ===\n",
    "sam = sam_model_registry[\"vit_h\"](\n",
    "    checkpoint=os.path.join(project_dir, \"models\", \"sam_vit_h_4b8939.pth\")\n",
    ").to(\"cuda\")\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "# === Processing ===\n",
    "for colormap_folder in os.listdir(images_base_dir):\n",
    "    if colormap_folder == \"optical\":\n",
    "        folder_path = os.path.join(images_base_dir, colormap_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        output_dir = os.path.join(project_dir, f\"{colormap_folder}_output_final2\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for file_path in glob.glob(f\"{folder_path}/*.jpg\"):\n",
    "            filename = os.path.basename(file_path)\n",
    "            image = cv2.imread(file_path)\n",
    "\n",
    "            if image.shape[1] != w or image.shape[0] != h:\n",
    "                image = cv2.resize(image, desired_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            enhanced_image = enhance_contrast(image_rgb)\n",
    "\n",
    "            masks = mask_generator.generate(enhanced_image)\n",
    "\n",
    "            combined_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "            for m in masks:\n",
    "                mask = m[\"segmentation\"].astype(np.uint8)\n",
    "                area_ratio = cv2.countNonZero(mask) / (h * w)\n",
    "\n",
    "                x, y, mask_w, mask_h = cv2.boundingRect(mask)\n",
    "                width_ratio = mask_w / w\n",
    "                height_ratio = mask_h / h\n",
    "                if 0.2 <= width_ratio <= 0.9 and height_ratio <= 0.9 and 0.15 <= area_ratio <= 0.8:\n",
    "                    combined_mask = cv2.bitwise_or(combined_mask, mask)\n",
    "\n",
    "            feathered_hands = feather_edges(enhanced_image, combined_mask, feather_amount=5)\n",
    "            output_hand_path = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_hands.jpg\")\n",
    "            cv2.imwrite(output_hand_path, cv2.cvtColor(feathered_hands, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            binary_mask_output = (combined_mask * 255).astype(np.uint8)\n",
    "            output_mask_path = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_mask.png\")\n",
    "            cv2.imwrite(output_mask_path, binary_mask_output)\n",
    "\n",
    "            print(f\"Saved combined hands and mask to: {output_hand_path}, {output_mask_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
    "\n",
    "# === Enhance contrast ===\n",
    "def enhance_contrast(image):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "# === Improved feathering ===\n",
    "def feather_edges(image, mask, feather_amount=5):\n",
    "    mask = mask.astype(np.float32)\n",
    "\n",
    "    # Dilate mask to preserve fingers and edges\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    mask_dilated = cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "    # Blur only the border by subtracting original from dilated\n",
    "    edge = mask_dilated - mask\n",
    "    edge_blur = cv2.GaussianBlur(edge, (0, 0), feather_amount)\n",
    "\n",
    "    # Create smooth transition alpha mask\n",
    "    soft_mask = np.clip(mask + edge_blur, 0, 1)\n",
    "    alpha = np.stack([soft_mask] * 3, axis=-1)\n",
    "\n",
    "    blended = (image * alpha).astype(np.uint8)\n",
    "    return blended\n",
    "\n",
    "def min_distance_to_corners(mask):\n",
    "    h, w = mask.shape\n",
    "    corners = np.array([\n",
    "        [0, 0],          # top-left\n",
    "        [0, w - 1],      # top-right\n",
    "        [h - 1, 0],      # bottom-left\n",
    "        [h - 1, w - 1]   # bottom-right\n",
    "    ])\n",
    "\n",
    "    # Get all non-zero (y, x) points in the mask\n",
    "    ys, xs = np.nonzero(mask)\n",
    "    points = np.stack([ys, xs], axis=1)\n",
    "\n",
    "    if len(points) == 0:\n",
    "        return np.inf  # if the mask is empty\n",
    "\n",
    "    # Compute distances from each point to each corner\n",
    "    dists = np.linalg.norm(points[:, None, :] - corners[None, :, :], axis=2)\n",
    "    dist = dists.min()  # closest any pixel gets to a corner\n",
    "\n",
    "    return dist\n",
    "\n",
    "# === Settings ===\n",
    "project_dir = \"/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "images_base_dir = os.path.join(project_dir, \"data\", \"images\")\n",
    "desired_size = (320, 240)\n",
    "w, h = desired_size[0], desired_size[1]\n",
    "min_area = 1000\n",
    "\n",
    "# === Load SAM ===\n",
    "sam = sam_model_registry[\"vit_h\"](\n",
    "    checkpoint=os.path.join(project_dir, \"models\", \"sam_vit_h_4b8939.pth\")\n",
    ").to(\"cuda\")\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "# === Loop through subfolders ===\n",
    "for colormap_folder in os.listdir(images_base_dir):\n",
    "    if colormap_folder == \"optical\":\n",
    "        folder_path = os.path.join(images_base_dir, colormap_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        output_dir = os.path.join(project_dir, f\"{colormap_folder}_output7\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"\\n📁 Processing folder: {colormap_folder}\")\n",
    "\n",
    "        for file_path in glob.glob(f\"{folder_path}/*.jpg\"):\n",
    "            filename = os.path.basename(file_path)\n",
    "            print(f\"\\n🔄 Processing image: {filename}\")\n",
    "\n",
    "            image = cv2.imread(file_path)\n",
    "            # Moved this line to save part\n",
    "            #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if image.shape[1] != desired_size[0] or image.shape[0] != desired_size[1]:\n",
    "                image = cv2.resize(image, desired_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            image = enhance_contrast(image)\n",
    "\n",
    "            masks = mask_generator.generate(image)\n",
    "\n",
    "            masks = masks[:2]\n",
    "            image_area = h * w\n",
    "            valid_masks = []\n",
    "            width_upper, width_lower, area_lower, area_higher = 0.9, 0.4, 0.2, 0.5\n",
    "\n",
    "            # mask1 and mask2 are binary masks (np.uint8) of the same shape\n",
    "            dist1 = min_distance_to_corners(masks[0][\"segmentation\"].astype(np.uint8))\n",
    "            dist2 = min_distance_to_corners(masks[1][\"segmentation\"].astype(np.uint8))\n",
    "            dist3 = min_distance_to_corners(masks[2][\"segmentation\"].astype(np.uint8))\n",
    "\n",
    "            if dist2 < dist3 and dist1 < dist3:\n",
    "                print(\"Mask 2 is furthest from a corner\")\n",
    "                valid_masks.append((2, masks[2][\"segmentation\"].astype(np.uint8)))\n",
    "            elif dist1 < dist2 and dist3 < dist2:\n",
    "                print(\"Mask 1 is furthest from a corner\")\n",
    "                valid_masks.append((1, masks[1][\"segmentation\"].astype(np.uint8)))\n",
    "            else:\n",
    "                print(\"Mask 0 is furthest from a corner\")\n",
    "                valid_masks.append((0, masks[0][\"segmentation\"].astype(np.uint8)))\n",
    "            # Filter masks based on area ratio\n",
    "            #for i, m in enumerate(masks):\n",
    "            #    mask = m[\"segmentation\"].astype(np.uint8)\n",
    "            #    area = cv2.countNonZero(mask)\n",
    "                #area_ratio = area / image_area\n",
    "\n",
    "                # === Width check ===\n",
    "                #x, y, mask_w, mask_h = cv2.boundingRect(mask)\n",
    "                #width_ratio = mask_w / w\n",
    "                #height_ratio = mask_h / h\n",
    "                #if width_ratio > width_upper or width_ratio < width_lower or height_ratio > 0.8:\n",
    "                    #continue\n",
    "\n",
    "                #if area_lower <= area_ratio <= area_higher:\n",
    "                # mask: binary mask of shape (height, width)\n",
    "            #    h, w = mask.shape\n",
    "\n",
    "                # Corners as (row, col) = (y, x)\n",
    "            #    corners = [\n",
    "            #        (0, 0), (0, w - 1),(h - 1, 0),(h - 1, w - 1)]\n",
    "\n",
    "                # Check if any corner is part of the mask\n",
    "                #corner_present = any(mask[y, x] > 0 for y, x in corners)\n",
    "\n",
    "                #if corner_present:\n",
    "                #    print(\"Mask touches a corner – might be background or invalid.\")\n",
    "                #else:\n",
    "                #    valid_masks.append((i, mask))\n",
    "                    #break\n",
    "\n",
    "            # Save the two valid masks\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            for i, (idx, mask) in enumerate(valid_masks):\n",
    "                feathered_hand = feather_edges(image, mask, feather_amount=5)\n",
    "                #side = \"left\" if i == 0 else \"right\"\n",
    "                output_path = os.path.join(output_dir, f\"{base_name}_hand.jpg\")\n",
    "                cv2.imwrite(output_path, cv2.cvtColor(feathered_hand, cv2.COLOR_RGB2BGR))\n",
    "                print(f\"✅ Saved hands (mask {idx}) to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "project_dir = \"/Users/Bunni/Documents/FinalProject/ThermoML\"\n",
    "input_dir = os.path.join(project_dir, \"gnuplot2_output\")\n",
    "output_dir = os.path.join(project_dir, \"gnuplot2_mediapipe_landmarks\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(project_dir, \"models\", \"hand_landmarker.task\")\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    num_hands=2\n",
    ")\n",
    "\n",
    "landmarker = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    if not file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        continue\n",
    "\n",
    "    image_path = os.path.join(input_dir, file)\n",
    "    image = cv2.imread(image_path)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)\n",
    "\n",
    "    result = landmarker.detect(mp_image)\n",
    "\n",
    "    if result.hand_landmarks:\n",
    "        for hand in result.hand_landmarks:\n",
    "            for landmark in hand:\n",
    "                x_px = int(landmark.x * image.shape[1])\n",
    "                y_px = int(landmark.y * image.shape[0])\n",
    "                cv2.circle(image, (x_px, y_px), 3, (0, 255, 0), -1)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"landmarks_{file}\")\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Saved landmark image to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
