{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we separate thermal pictures that were extracted using the thermal data with the cv2 inferno color using the Meta SAM model. We get a segmentation of each hand separately.\n",
    "\n",
    "We achieve successful(both hands) segmentation at 70%. This rate includes cold hands that struggle to be detected, excluding cold hands the rate is bigger around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# === Enhance contrast ===\n",
    "def enhance_contrast(image):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "# === Improved feathering ===\n",
    "def feather_edges(image, mask, feather_amount=5):\n",
    "    mask = mask.astype(np.float32)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    mask_dilated = cv2.dilate(mask, kernel, iterations=1)\n",
    "    edge = mask_dilated - mask\n",
    "    edge_blur = cv2.GaussianBlur(edge, (0, 0), feather_amount)\n",
    "    soft_mask = np.clip(mask + edge_blur, 0, 1)\n",
    "    alpha = np.stack([soft_mask] * 3, axis=-1)\n",
    "    blended = (image * alpha).astype(np.uint8)\n",
    "    return blended\n",
    "\n",
    "# === Settings ===\n",
    "project_dir = \"\"\n",
    "inferno_dir = os.path.join(project_dir, \"data\", \"images\", \"inferno\")\n",
    "desired_size = (224, 224)\n",
    "min_area = 1000\n",
    "\n",
    "# === Load SAM ===\n",
    "sam = sam_model_registry[\"vit_h\"](\n",
    "    checkpoint=os.path.join(project_dir, \"models\", \"sam_vit_h_4b8939.pth\")\n",
    ")\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# === Output directory ===\n",
    "output_dir = os.path.join(project_dir, \"inferno_output\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"\\n Processing inferno images in: {inferno_dir}\")\n",
    "\n",
    "# === Process only inferno images ===\n",
    "for file_path in glob.glob(f\"{inferno_dir}/*.jpg\"):\n",
    "    filename = os.path.basename(file_path)\n",
    "    print(f\"\\n Processing image: {filename}\")\n",
    "\n",
    "    image = cv2.imread(file_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if image.shape[1] != desired_size[0] or image.shape[0] != desired_size[1]:\n",
    "        image = cv2.resize(image, desired_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    image = enhance_contrast(image)\n",
    "    predictor.set_image(image)\n",
    "\n",
    "    h, w = desired_size[1], desired_size[0]\n",
    "    input_point = np.array([\n",
    "        [int(w * 0.25), int(h * 0.6)],\n",
    "        [int(w * 0.75), int(h * 0.6)],\n",
    "        [int(w * 0.25), int(h * 0.4)],\n",
    "        [int(w * 0.75), int(h * 0.4)],\n",
    "        [int(w * 0.5), int(h * 0.85)]\n",
    "    ])\n",
    "    input_label = np.array([1, 1, 1, 1, 0])\n",
    "\n",
    "    masks, _, _ = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=False\n",
    "    )\n",
    "\n",
    "    mask = masks[0].astype(np.uint8)\n",
    "    num_labels, labels = cv2.connectedComponents(mask)\n",
    "    print(f\" Found {num_labels - 1} components.\")\n",
    "\n",
    "    hands = []\n",
    "    for label in range(1, num_labels):\n",
    "        hand_mask = (labels == label).astype(np.uint8)\n",
    "        area = cv2.countNonZero(hand_mask)\n",
    "        if area < min_area:\n",
    "            continue\n",
    "        M = cv2.moments(hand_mask)\n",
    "        if M[\"m00\"] == 0:\n",
    "            continue\n",
    "        center_x = int(M[\"m10\"] / M[\"m00\"])\n",
    "        hands.append((center_x, hand_mask))\n",
    "\n",
    "    if len(hands) == 1:\n",
    "        print(\"Only 1 component found attempting to split\")\n",
    "        big_mask = hands[0][1]\n",
    "        mid_x = w // 2\n",
    "        left_mask = np.zeros_like(big_mask)\n",
    "        right_mask = np.zeros_like(big_mask)\n",
    "        left_mask[:, :mid_x] = big_mask[:, :mid_x]\n",
    "        right_mask[:, mid_x:] = big_mask[:, mid_x:]\n",
    "        if cv2.countNonZero(left_mask) > min_area and cv2.countNonZero(right_mask) > min_area:\n",
    "            hands = [(int(w * 0.25), left_mask), (int(w * 0.75), right_mask)]\n",
    "        else:\n",
    "            print(\"Failed to split  skipping image.\")\n",
    "            continue\n",
    "\n",
    "    if len(hands) != 2:\n",
    "        print(\"Skipping did not find exactly 2 hands.\")\n",
    "        continue\n",
    "\n",
    "    hands.sort(key=lambda h: h[0])\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "\n",
    "    for i, (center_x, hand_mask) in enumerate(hands):\n",
    "        side = \"left\" if i == 0 else \"right\"\n",
    "        feathered_hand = feather_edges(image, hand_mask, feather_amount=5)\n",
    "        output_path = os.path.join(output_dir, f\"{base_name}_{side}_hand.jpg\")\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(feathered_hand, cv2.COLOR_RGB2BGR))\n",
    "        print(f\" Saved {side} hand to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we separate the segmented hands that we got from the SAM into the joints, and the tips of the fingers using the google HandLandMarker model.\n",
    "\n",
    "We achieve successful joint recognition at 100% of all successful segmented hands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# === Paths ===\n",
    "project_dir = \"\"\n",
    "input_dir = os.path.join(project_dir, \"inferno_output\")\n",
    "output_dir = os.path.join(project_dir, \"inferno_landmarks\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Load MediaPipe model ===\n",
    "model_path = os.path.join(project_dir, \"models\", \"hand_landmarker.task\")\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    num_hands=2\n",
    ")\n",
    "\n",
    "landmarker = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# === Process each image ===\n",
    "for file in os.listdir(input_dir):\n",
    "    if not file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        continue\n",
    "\n",
    "    image_path = os.path.join(input_dir, file)\n",
    "    image = cv2.imread(image_path)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)\n",
    "\n",
    "    result = landmarker.detect(mp_image)\n",
    "\n",
    "    if result.hand_landmarks:\n",
    "        for hand in result.hand_landmarks:\n",
    "            for landmark in hand:\n",
    "                x_px = int(landmark.x * image.shape[1])\n",
    "                y_px = int(landmark.y * image.shape[0])\n",
    "                cv2.circle(image, (x_px, y_px), 3, (0, 255, 0), -1)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"landmarks_{file}\")\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Saved landmark image to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code also adds the center of the hand based on the joints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# === Paths ===\n",
    "project_dir = \"\"\n",
    "input_dir = os.path.join(project_dir, \"inferno_output\")\n",
    "output_dir = os.path.join(project_dir, \"inferno_landmarks\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Load MediaPipe model ===\n",
    "model_path = os.path.join(project_dir, \"models\", \"hand_landmarker.task\")\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = vision.HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    num_hands=2\n",
    ")\n",
    "landmarker = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# === Utilities ===\n",
    "def to_pixel(landmark, image_shape):\n",
    "    h, w = image_shape[:2]\n",
    "    return int(landmark.x * w), int(landmark.y * h)\n",
    "\n",
    "def intersect(p1, p2, p3, p4):\n",
    "    a1 = p2[1] - p1[1]\n",
    "    b1 = p1[0] - p2[0]\n",
    "    c1 = a1 * p1[0] + b1 * p1[1]\n",
    "\n",
    "    a2 = p4[1] - p3[1]\n",
    "    b2 = p3[0] - p4[0]\n",
    "    c2 = a2 * p3[0] + b2 * p3[1]\n",
    "\n",
    "    determinant = a1 * b2 - a2 * b1\n",
    "    if determinant == 0:\n",
    "        return None\n",
    "    x = (b2 * c1 - b1 * c2) / determinant\n",
    "    y = (a1 * c2 - a2 * c1) / determinant\n",
    "    return int(x), int(y)\n",
    "\n",
    "# === Main loop ===\n",
    "for file in os.listdir(input_dir):\n",
    "    if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue\n",
    "\n",
    "    image_path = os.path.join(input_dir, file)\n",
    "    image = cv2.imread(image_path)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)\n",
    "\n",
    "    result = landmarker.detect(mp_image)\n",
    "\n",
    "    if result.hand_landmarks:\n",
    "        for hand_landmarks in result.hand_landmarks:\n",
    "            pts = {\n",
    "                \"thumb\": to_pixel(hand_landmarks[1], image.shape),\n",
    "                \"pinky\": to_pixel(hand_landmarks[17], image.shape),\n",
    "                \"middle\": to_pixel(hand_landmarks[12], image.shape),\n",
    "                \"wrist\": to_pixel(hand_landmarks[0], image.shape)\n",
    "            }\n",
    "            center = intersect(pts[\"thumb\"], pts[\"pinky\"], pts[\"wrist\"], pts[\"middle\"])\n",
    "            if center:\n",
    "                cv2.drawMarker(image, center, (255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=10, thickness=2)\n",
    "\n",
    "            for landmark in hand_landmarks:\n",
    "                x, y = to_pixel(landmark, image.shape)\n",
    "                cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"landmarks_{file}\")\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Saved with palm center marked: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we separate optical pictures that were extracted using the Meta SAM model with a slightly different algorithm. We get a segmentation of both hands together.\n",
    "\n",
    "We achieve successful segmentation at 78%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
    "\n",
    "# === Load SAM ===\n",
    "sam = sam_model_registry[\"vit_h\"](\n",
    "    checkpoint=os.path.join(project_dir, \"models\", \"sam_vit_h_4b8939.pth\")\n",
    ")\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# === Enhance contrast ===\n",
    "def enhance_contrast(image):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "# === Improved feathering ===\n",
    "def feather_edges(image, mask, feather_amount=5):\n",
    "    mask = mask.astype(np.float32)\n",
    "\n",
    "    # Dilate mask to preserve fingers and edges\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    mask_dilated = cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "    # Blur only the border by subtracting original from dilated\n",
    "    edge = mask_dilated - mask\n",
    "    edge_blur = cv2.GaussianBlur(edge, (0, 0), feather_amount)\n",
    "\n",
    "    # Create smooth transition alpha mask\n",
    "    soft_mask = np.clip(mask + edge_blur, 0, 1)\n",
    "    alpha = np.stack([soft_mask] * 3, axis=-1)\n",
    "\n",
    "    blended = (image * alpha).astype(np.uint8)\n",
    "    return blended\n",
    "\n",
    "def average_min_distance_to_corners_with_center(mask):\n",
    "    h, w = mask.shape\n",
    "    corners = np.array([\n",
    "        [0, 0],            # top-left\n",
    "        [0, w - 1],        # top-right\n",
    "        [h // 2, 0],       # middle-left\n",
    "        [h // 2, w - 1],   # middle-right\n",
    "        [0, w // 2],       # top-center\n",
    "        [0, w // 2]        # top-center again to weight it more\n",
    "    ])\n",
    "\n",
    "    ys, xs = np.nonzero(mask)\n",
    "    if len(ys) == 0:\n",
    "        return -np.inf  # totally empty mask\n",
    "\n",
    "    points = np.stack([ys, xs], axis=1)\n",
    "\n",
    "    # === Corner Score ===\n",
    "    min_dists = [np.min(np.linalg.norm(points - corner, axis=1)) for corner in corners]\n",
    "    score = np.sum(min_dists)\n",
    "\n",
    "    # === Edge Penalty ===\n",
    "    if np.any(mask[0, :]) or np.any(mask[:h // 2, 0]) or np.any(mask[:h // 2, w - 1]): \n",
    "        score *= 0.5\n",
    "        \n",
    "    # === Center Proximity Score ===\n",
    "    mask_center = np.mean(points, axis=0)  # (y, x)\n",
    "    image_center = np.array([h / 2, w / 2])\n",
    "    avg_center_dist = np.linalg.norm(mask_center - image_center)\n",
    "\n",
    "    # Max possible distance = image diagonal (for normalization)\n",
    "    max_dist = np.linalg.norm([h / 2, w / 2])\n",
    "    center_score = (1 + (avg_center_dist / max_dist)) * score  # proportional weight\n",
    "\n",
    "    # === Final Score ===\n",
    "    final_score = score + center_score\n",
    "    return final_score\n",
    "\n",
    "def calc_mask(masks, width_upper, width_lower, area_lower, area_higher):\n",
    "    # Filter masks based on area ratio\n",
    "    valid_masks, valid_dist = [], []\n",
    "    for i, m in enumerate(masks):\n",
    "        mask = m[\"segmentation\"].astype(np.uint8)\n",
    "        area = cv2.countNonZero(mask)\n",
    "        area_ratio = area / image_area\n",
    "\n",
    "        # === Width check ===\n",
    "        x, y, mask_w, mask_h = cv2.boundingRect(mask)\n",
    "        width_ratio = mask_w / w\n",
    "        height_ratio = mask_h / h\n",
    "        if width_ratio > width_upper or width_ratio < width_lower or height_ratio > 0.9:\n",
    "            continue\n",
    "        if area_lower <= area_ratio <= area_higher:\n",
    "            valid_masks.append((i, mask))\n",
    "            valid_dist.append(average_min_distance_to_corners_with_center(mask))\n",
    "    return valid_masks, valid_dist\n",
    "\n",
    "# === Settings ===\n",
    "project_dir =  \"\"\n",
    "images_base_dir = os.path.join(project_dir, \"data\", \"images\")\n",
    "desired_size = (320, 240)\n",
    "w, h = desired_size[0], desired_size[1]\n",
    "min_area = 1000\n",
    "\n",
    "# === Loop through subfolders ===\n",
    "for colormap_folder in os.listdir(images_base_dir):\n",
    "    if colormap_folder == \"optical\":\n",
    "        folder_path = os.path.join(images_base_dir, colormap_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        output_dir = os.path.join(project_dir, f\"{colormap_folder}_output8\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"\\n Processing folder: {colormap_folder}\")\n",
    "\n",
    "        for file_path in glob.glob(f\"{folder_path}/*.jpg\"):\n",
    "            filename = os.path.basename(file_path)\n",
    "            print(f\"\\n Processing image: {filename}\")\n",
    "\n",
    "            image = cv2.imread(file_path)\n",
    "            # Moved this line to save part\n",
    "            #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if image.shape[1] != desired_size[0] or image.shape[0] != desired_size[1]:\n",
    "                image = cv2.resize(image, desired_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            image = enhance_contrast(image)\n",
    "\n",
    "            masks = mask_generator.generate(image)\n",
    "\n",
    "            image_area = h * w\n",
    "            valid_masks, valid_dist = [], []\n",
    "            width_upper, width_lower, area_lower, area_higher = 0.9, 0.2, 0.12, 0.8\n",
    "            valid_masks, valid_dist = calc_mask(masks, width_upper, width_lower, area_lower, area_higher)\n",
    "            save_mask = []\n",
    "            max_dist = -1\n",
    "            best_index = -1\n",
    "\n",
    "            for idx in range(len(valid_dist)):\n",
    "                if valid_dist[idx] > max_dist:\n",
    "                    max_dist = valid_dist[idx]\n",
    "                    best_index = idx\n",
    "\n",
    "            if best_index != -1:\n",
    "                save_mask.append(valid_masks[best_index])\n",
    "\n",
    "            # Save the two valid masks\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            for i, (idx, mask) in enumerate(save_mask):\n",
    "                feathered_hand = feather_edges(image, mask, feather_amount=5)\n",
    "                output_path = os.path.join(output_dir, f\"{base_name}_hands.jpg\")\n",
    "                cv2.imwrite(output_path, cv2.cvtColor(feathered_hand, cv2.COLOR_RGB2BGR))\n",
    "                print(f\"Saved hands (mask {idx}) to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
